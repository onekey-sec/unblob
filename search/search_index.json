{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":""},{"location":"#what-is-unblob","title":"What is unblob?","text":"<p>unblob is an accurate, fast, and easy-to-use extraction suite. It parses unknown binary blobs for more than 30 different archive, compression, and file-system formats, extracts their content recursively, and carves out unknown chunks that have not been accounted for.</p> <p>unblob is free to use, licensed under MIT license, it has a command line interface and can be used as a Python library. This turns unblob into the perfect companion for extracting, analyzing, and reverse engineering firmware images.</p> <p>unblob was originally developed and currently maintained by ONEKEY and it is used in production in our ONEKEY analysis platform.</p>"},{"location":"#demo","title":"Demo","text":""},{"location":"#why-unblob","title":"Why unblob?","text":"<p>One of the major challenges of embedded security analysis is the sound and safe extraction of arbitrary firmware.</p> <p>Specialized tools that can extract information from those firmware images already exist, but we were carving for something smarter that could identify both start-offset and end-offset of a specific chunk (e.g. filesystem, compression stream, archive, ...) as well as handle formats split across multiple files.</p> <p>We stick to the format standard as much as possible when deriving these offsets, and we clearly define what we want out of identified chunks (e.g., not extracting meta-data to disk, padding removal). This strategy helps us feed known valid data to extractors and precisely identify chunks, turning unknown unknowns into known unknowns.</p> <p>Given the modular design of unblob and the ever-expanding repository of supported formats, unblob could very well be used in areas outside embedded security such as data recovery, memory forensics, or malware analysis.</p>"},{"location":"#our-objectives","title":"Our Objectives","text":"<p>unblob has been developed with the following objectives in mind:</p> <ul> <li> <p>Accuracy - chunk start offsets are identified using battle tested rules,   while end offsets are computed according to the format's standard without   deviating from it. We minimize false positives as much as possible by   validating header structures and discarding overlapping chunks.</p> </li> <li> <p>Security - unblob does not require elevated privileges to run. It's   heavily tested and has been fuzz   tested   against a large corpus of files and firmware images. We rely on up-to-date   third party dependencies that are   locked to limit   potential supply chain issues. We use safe extractors that we audited and   fixed where required (see path traversal in   ubi_reader,   path traversal in   jefferson,   integer overflow in Yara).</p> </li> <li> <p>Extensibility - unblob exposes an API that can be used to write custom   format handlers and   extractors in no time.</p> </li> <li> <p>Speed - we want unblob to be blazing fast, that's why we use   multi-processing by default, make sure to write efficient code, use   memory-mapped files, and use   Hyperscan as a   high-performance matching library. Computation-intensive functions   are written in Rust and called from Python using specific bindings.</p> </li> </ul>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>unblob identifies known and unknown chunks of data within a file:</p> <ul> <li> <p>known chunks are identified by finding the start offset using a search   rule, and the end offset is computed based on the format standard. Unknown   chunks represents unidentified chunks of data before, after, or between   known chunks. Unknown chunks composed of known content (e.g., null padding,   <code>0xFF</code> padding) are identified automatically and reported as such.</p> </li> <li> <p>unblob will carve out known chunks to disk and perform the extraction   phase using the extractor assigned to a given handler. It will then walk the   extracted content, looking for chunks in extracted files.</p> </li> <li> <p>a report on metadata can be generated by unblob, providing detailed   information about identified chunks (format, offsets, size, entropy) and their   extracted content if available (ownership, permissions, timestamps, ...).</p> </li> </ul> <p></p> <p>unblob also supports special formats where data is split across multiple files like multi-volume archives or data &amp; meta-data formats:</p> <ul> <li> <p>Special DirectoryHandler is responsible to identify the files that make up a multi files set.</p> </li> <li> <p>Identified MultiFile sets are not carved, but rather directly extracted using special DirectoryExtractor.</p> </li> </ul>"},{"location":"#used-technologies","title":"Used technologies","text":"<ul> <li>unblob is written in Python.</li> <li>For quickly searching binary patterns in files, we use Hyperscan.</li> <li>For extracting recognized formats, we use all kinds of different Extractors.</li> <li>For ELF analysis, we are using LIEF with   its Python bindings.</li> <li>For CPU-intensive tasks (e.g. entropy calculation), we use   Rust to speed things up.</li> <li>For the pretty command line interface, we are using the   Click library.</li> <li> <p>For structured logging, we are using the   structlog library.</p> </li> <li> <p>For development and testing tools, see the Development page.</p> </li> </ul>"},{"location":"#license","title":"License","text":"<p>unblob is licensed under the permissive MIT license, so you can use it without restrictions.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#unblob.models.Handler","title":"<code>unblob.models.Handler</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[TExtractor]</code></p> <p>A file type handler is responsible for searching, validating and \"unblobbing\" files from Blobs.</p> Source code in <code>python/unblob/models.py</code> <pre><code>class Handler(abc.ABC, Generic[TExtractor]):\n    \"\"\"A file type handler is responsible for searching, validating and \"unblobbing\" files from Blobs.\"\"\"\n\n    NAME: str\n    PATTERNS: list[Pattern]\n    # We need this, because not every match reflects the actual start\n    # (e.g. tar magic is in the middle of the header)\n    PATTERN_MATCH_OFFSET: int = 0\n\n    EXTRACTOR: TExtractor\n\n    DOC: Union[HandlerDoc, None]\n\n    @classmethod\n    def get_dependencies(cls):\n        \"\"\"Return external command dependencies needed for this handler to work.\"\"\"\n        if cls.EXTRACTOR is not None:\n            return cls.EXTRACTOR.get_dependencies()\n        return []\n\n    @abc.abstractmethod\n    def calculate_chunk(self, file: File, start_offset: int) -&gt; Optional[ValidChunk]:\n        \"\"\"Calculate the Chunk offsets from the File and the file type headers.\"\"\"\n\n    def extract(self, inpath: Path, outdir: Path) -&gt; Optional[ExtractResult]:\n        if self.EXTRACTOR is None:\n            logger.debug(\"Skipping file: no extractor.\", path=inpath)\n            raise ExtractError\n\n        # We only extract every blob once, it's a mistake to extract the same blob again\n        outdir.mkdir(parents=True, exist_ok=False)\n\n        return self.EXTRACTOR.extract(inpath, outdir)\n</code></pre>"},{"location":"api/#unblob.models.Handler.get_dependencies","title":"<code>get_dependencies()</code>  <code>classmethod</code>","text":"<p>Return external command dependencies needed for this handler to work.</p> Source code in <code>python/unblob/models.py</code> <pre><code>@classmethod\ndef get_dependencies(cls):\n    \"\"\"Return external command dependencies needed for this handler to work.\"\"\"\n    if cls.EXTRACTOR is not None:\n        return cls.EXTRACTOR.get_dependencies()\n    return []\n</code></pre>"},{"location":"api/#unblob.models.Handler.calculate_chunk","title":"<code>calculate_chunk(file, start_offset)</code>  <code>abstractmethod</code>","text":"<p>Calculate the Chunk offsets from the File and the file type headers.</p> Source code in <code>python/unblob/models.py</code> <pre><code>@abc.abstractmethod\ndef calculate_chunk(self, file: File, start_offset: int) -&gt; Optional[ValidChunk]:\n    \"\"\"Calculate the Chunk offsets from the File and the file type headers.\"\"\"\n</code></pre>"},{"location":"api/#unblob.models.Handler.extract","title":"<code>extract(inpath, outdir)</code>","text":"Source code in <code>python/unblob/models.py</code> <pre><code>def extract(self, inpath: Path, outdir: Path) -&gt; Optional[ExtractResult]:\n    if self.EXTRACTOR is None:\n        logger.debug(\"Skipping file: no extractor.\", path=inpath)\n        raise ExtractError\n\n    # We only extract every blob once, it's a mistake to extract the same blob again\n    outdir.mkdir(parents=True, exist_ok=False)\n\n    return self.EXTRACTOR.extract(inpath, outdir)\n</code></pre>"},{"location":"api/#unblob.models.StructHandler","title":"<code>unblob.models.StructHandler</code>","text":"<p>               Bases: <code>Handler</code></p> Source code in <code>python/unblob/models.py</code> <pre><code>class StructHandler(Handler):\n    C_DEFINITIONS: str\n    # A struct from the C_DEFINITIONS used to parse the file's header\n    HEADER_STRUCT: str\n\n    def __init__(self):\n        self._struct_parser = StructParser(self.C_DEFINITIONS)\n\n    @property\n    def cparser_le(self):\n        return self._struct_parser.cparser_le\n\n    @property\n    def cparser_be(self):\n        return self._struct_parser.cparser_be\n\n    def parse_header(self, file: File, endian=Endian.LITTLE):\n        header = self._struct_parser.parse(self.HEADER_STRUCT, file, endian)\n        logger.debug(\"Header parsed\", header=header, _verbosity=3)\n        return header\n</code></pre>"},{"location":"api/#unblob.models.StructHandler.parse_header","title":"<code>parse_header(file, endian=Endian.LITTLE)</code>","text":"Source code in <code>python/unblob/models.py</code> <pre><code>def parse_header(self, file: File, endian=Endian.LITTLE):\n    header = self._struct_parser.parse(self.HEADER_STRUCT, file, endian)\n    logger.debug(\"Header parsed\", header=header, _verbosity=3)\n    return header\n</code></pre>"},{"location":"api/#unblob.models.DirectoryHandler","title":"<code>unblob.models.DirectoryHandler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>A directory type handler is responsible for searching, validating and \"unblobbing\" files from multiple files in a directory.</p> Source code in <code>python/unblob/models.py</code> <pre><code>class DirectoryHandler(abc.ABC):\n    \"\"\"A directory type handler is responsible for searching, validating and \"unblobbing\" files from multiple files in a directory.\"\"\"\n\n    NAME: str\n\n    EXTRACTOR: DirectoryExtractor\n\n    PATTERN: DirectoryPattern\n\n    DOC: Union[HandlerDoc, None]\n\n    @classmethod\n    def get_dependencies(cls):\n        \"\"\"Return external command dependencies needed for this handler to work.\"\"\"\n        if cls.EXTRACTOR:\n            return cls.EXTRACTOR.get_dependencies()\n        return []\n\n    @abc.abstractmethod\n    def calculate_multifile(self, file: Path) -&gt; Optional[MultiFile]:\n        \"\"\"Calculate the MultiFile in a directory, using a file matched by the pattern as a starting point.\"\"\"\n\n    def extract(self, paths: list[Path], outdir: Path) -&gt; Optional[ExtractResult]:\n        if self.EXTRACTOR is None:\n            logger.debug(\"Skipping file: no extractor.\", paths=paths)\n            raise ExtractError\n\n        # We only extract every blob once, it's a mistake to extract the same blob again\n        outdir.mkdir(parents=True, exist_ok=False)\n\n        return self.EXTRACTOR.extract(paths, outdir)\n</code></pre>"},{"location":"api/#unblob.models.DirectoryHandler.get_dependencies","title":"<code>get_dependencies()</code>  <code>classmethod</code>","text":"<p>Return external command dependencies needed for this handler to work.</p> Source code in <code>python/unblob/models.py</code> <pre><code>@classmethod\ndef get_dependencies(cls):\n    \"\"\"Return external command dependencies needed for this handler to work.\"\"\"\n    if cls.EXTRACTOR:\n        return cls.EXTRACTOR.get_dependencies()\n    return []\n</code></pre>"},{"location":"api/#unblob.models.DirectoryHandler.calculate_multifile","title":"<code>calculate_multifile(file)</code>  <code>abstractmethod</code>","text":"<p>Calculate the MultiFile in a directory, using a file matched by the pattern as a starting point.</p> Source code in <code>python/unblob/models.py</code> <pre><code>@abc.abstractmethod\ndef calculate_multifile(self, file: Path) -&gt; Optional[MultiFile]:\n    \"\"\"Calculate the MultiFile in a directory, using a file matched by the pattern as a starting point.\"\"\"\n</code></pre>"},{"location":"api/#unblob.models.DirectoryHandler.extract","title":"<code>extract(paths, outdir)</code>","text":"Source code in <code>python/unblob/models.py</code> <pre><code>def extract(self, paths: list[Path], outdir: Path) -&gt; Optional[ExtractResult]:\n    if self.EXTRACTOR is None:\n        logger.debug(\"Skipping file: no extractor.\", paths=paths)\n        raise ExtractError\n\n    # We only extract every blob once, it's a mistake to extract the same blob again\n    outdir.mkdir(parents=True, exist_ok=False)\n\n    return self.EXTRACTOR.extract(paths, outdir)\n</code></pre>"},{"location":"api/#unblob.models.Extractor","title":"<code>unblob.models.Extractor</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>python/unblob/models.py</code> <pre><code>class Extractor(abc.ABC):\n    def get_dependencies(self) -&gt; list[str]:\n        \"\"\"Return the external command dependencies.\"\"\"\n        return []\n\n    @abc.abstractmethod\n    def extract(self, inpath: Path, outdir: Path) -&gt; Optional[ExtractResult]:\n        \"\"\"Extract the carved out chunk.\n\n        Raises ExtractError on failure.\n        \"\"\"\n</code></pre>"},{"location":"api/#unblob.models.Extractor.get_dependencies","title":"<code>get_dependencies()</code>","text":"<p>Return the external command dependencies.</p> Source code in <code>python/unblob/models.py</code> <pre><code>def get_dependencies(self) -&gt; list[str]:\n    \"\"\"Return the external command dependencies.\"\"\"\n    return []\n</code></pre>"},{"location":"api/#unblob.models.Extractor.extract","title":"<code>extract(inpath, outdir)</code>  <code>abstractmethod</code>","text":"<p>Extract the carved out chunk.</p> <p>Raises ExtractError on failure.</p> Source code in <code>python/unblob/models.py</code> <pre><code>@abc.abstractmethod\ndef extract(self, inpath: Path, outdir: Path) -&gt; Optional[ExtractResult]:\n    \"\"\"Extract the carved out chunk.\n\n    Raises ExtractError on failure.\n    \"\"\"\n</code></pre>"},{"location":"api/#unblob.models.DirectoryExtractor","title":"<code>unblob.models.DirectoryExtractor</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>python/unblob/models.py</code> <pre><code>class DirectoryExtractor(abc.ABC):\n    def get_dependencies(self) -&gt; list[str]:\n        \"\"\"Return the external command dependencies.\"\"\"\n        return []\n\n    @abc.abstractmethod\n    def extract(self, paths: list[Path], outdir: Path) -&gt; Optional[ExtractResult]:\n        \"\"\"Extract from a multi file path list.\n\n        Raises ExtractError on failure.\n        \"\"\"\n</code></pre>"},{"location":"api/#unblob.models.DirectoryExtractor.get_dependencies","title":"<code>get_dependencies()</code>","text":"<p>Return the external command dependencies.</p> Source code in <code>python/unblob/models.py</code> <pre><code>def get_dependencies(self) -&gt; list[str]:\n    \"\"\"Return the external command dependencies.\"\"\"\n    return []\n</code></pre>"},{"location":"api/#unblob.models.DirectoryExtractor.extract","title":"<code>extract(paths, outdir)</code>  <code>abstractmethod</code>","text":"<p>Extract from a multi file path list.</p> <p>Raises ExtractError on failure.</p> Source code in <code>python/unblob/models.py</code> <pre><code>@abc.abstractmethod\ndef extract(self, paths: list[Path], outdir: Path) -&gt; Optional[ExtractResult]:\n    \"\"\"Extract from a multi file path list.\n\n    Raises ExtractError on failure.\n    \"\"\"\n</code></pre>"},{"location":"development/","title":"Development","text":"<p>Want to contribute to unblob? That's great! We developed a framework (we sometimes reference it as \"unblob core\"), to make it very easy to add support for new file formats. This page describes the process of how to do that.</p> <p>If you don't want or don't know how to develop complex Python applications, that's not a problem! If there is a format you would like to be supported in unblob and you can describe and explain it (maybe with nifty hex-representations, hand-drawings or smoke signs, or whatever you cup-of-tea is), we might help you implement it! Just open a new ticket in the GitHub issue tracker.</p> <p>If you do know all this stuff, and you have all the tools in the world installed, you can just jump to the How to write handlers section where the exciting stuff is.</p>"},{"location":"development/#setting-up-development-environment","title":"Setting up development environment","text":""},{"location":"development/#required-tools","title":"Required tools","text":"<ul> <li> <p>Python: unblob requires Python 3.9 or above. Make sure that   Python is installed on your system.</p> </li> <li> <p>git: You need it for cloning the repository.   Install it from the git-scm website.</p> </li> <li> <p>uv: it is a package manager for Python dependencies. Follow the instructions on the   uv website to install the latest version.</p> </li> <li> <p>pre-commit: We are using pre-commit to run   checks like linters, type checks and formatting issues.</p> </li> <li> <p>Git LFS: We have big integration test files, and we are using Git LFS to track them.   Install <code>git-lfs</code> from the website.</p> </li> <li> <p>Rust some functionality of unblob is implemented in Rust. Building it requires a Rust toolchain   (e.g. via <code>rustup</code>) to be installed on the host system. Follow the   instructions on the rustup website to install it.</p> </li> <li> <p>pyenv (Recommended): When you are working with multiple versions of Python,   pyenv makes it very easy to install and use different versions and make virtualenvs.   Follow the instructions on GitHub for the installation.   If your system already has at least Python 3.9 installed, you don't need it.</p> </li> </ul>"},{"location":"development/#cloning-the-git-repository","title":"Cloning the Git repository","text":"<p>Set up your git config, fork the project on GitHub, then clone your fork locally.</p> <p>If you installed <code>pre-commit</code>, you can run <code>pre-commit install</code>, which makes pre-commit run automatically during git commits with git hooks, so you don't have to run them manually.</p> <p>You need to setup Git LFS once, before you will be able to run the whole test suite:</p> <pre><code>git lfs install\n</code></pre> <p>Warning</p> <p>If you have cloned the repository prior to installing Git LFS, you need to run the following commands in the cloned repository once:</p> <pre><code>git lfs pull\ngit lfs checkout\n</code></pre>"},{"location":"development/#making-a-virtualenv","title":"Making a virtualenv","text":"<p>The recommended way to develop Python projects in a semi-isolated way is to use <code>virtualenv</code>.</p> <p>If you don't want to manage it separately, you can rely on <code>uv</code> to automatically create a virtualenv for you on install.</p> <p>Or instead of uv you can use <code>pyenv</code>. You can set the Python interpreter version for the local folder only with:</p> <pre><code>pyenv local 3.12.7\n</code></pre>"},{"location":"development/#installing-python-dependencies","title":"Installing Python dependencies","text":"<p>We are using uv to manage our Python dependencies. To install all required dependencies for development, you can run the following command:</p> <pre><code>uv sync --all-extras --dev\n</code></pre> <p>Please note that it installs dependencies within the dedicated virtual environment. So if you want to run <code>unblob</code> or <code>pytest</code>, you need to do it from within the virtual environment:</p> <p>Using uv run:</p> <pre><code>uv run unblob\nuv run pytest tests -v\n</code></pre> <p>By dropping into the virtual environment:</p> <pre><code>uv run $SHELL\nunblob\npytest tests -v\n</code></pre>"},{"location":"development/#running-pre-commit","title":"Running pre-commit","text":"<p>If you installed the <code>pre-commit</code> git hook when setting up your local git repo, you don't need this step, otherwise you can run all checks with <code>pre-commit run --all-files</code>.</p>"},{"location":"development/#running-the-tests","title":"Running the tests","text":"<p>We are using pytest for running our test suite. We have big integration files in the <code>tests/integration</code> directory, we are using Git LFS to track them. Only after you installed Git LFS, can you run all tests, with <code>python -m pytest tests/</code> in the activated virtualenv.</p>"},{"location":"development/#writing-handlers","title":"Writing handlers","text":"<p>Every handler inherits from the abstract class <code>Handler</code> located in unblob/models.py:</p> <pre><code>class Handler(abc.ABC):\n    \"\"\"A file type handler is responsible for searching, validating and \"unblobbing\" files from Blobs.\"\"\"\n    NAME: str\n    PATTERNS: str\n    PATTERN_MATCH_OFFSET: int = 0\n    EXTRACTOR: Optional[Extractor]\n\n    @classmethod\n    def get_dependencies(cls):\n        \"\"\"Returns external command dependencies needed for this handler to work.\"\"\"\n\n    @abc.abstractmethod\n    def calculate_chunk(self, file: io.BufferedIOBase, start_offset: int) -&gt; Optional[ValidChunk]:\n        \"\"\"Returns a ValidChunk when it found a valid format for this Handler.\n        Otherwise it can raise and Exception or return None, those will be ignored.\n        \"\"\"\n\n    def extract(self, inpath: Path, outdir: Path):\n        \"\"\"Responsible for extraction a ValidChunk.\"\"\"\n</code></pre> <ul> <li><code>NAME</code>: a unique name for this handler, this value will be appended at the end of carved out chunks</li> <li><code>PATTERNS</code>: an array of <code>Hyperscan</code> rules.</li> <li><code>PATTERN_MATCH_OFFSET</code>: an offset from the <code>hyperscan</code> match to the actual start offset.   This happens when the magic is not the first field in a file header</li> <li><code>EXTRACTOR</code>: an optional Extractor. It can be set to <code>None</code>   if the handler is supposed to only carve files</li> <li><code>get_dependencies()</code>: returns the extractor dependencies. This helps unblob keep   track of third party dependencies.</li> <li><code>calculate_chunk()</code>: this is the method that needs to be overridden in your   handler. It receives a <code>file</code> object and the effective <code>start_offset</code> of your   chunk. This is where you implement the logic to compute the <code>end_offset</code> and   return a <code>ValidChunk</code> object.</li> </ul>"},{"location":"development/#structhandler-class","title":"StructHandler class","text":"<p><code>StructHandler</code> is a specialized subclass of <code>Handler</code> that provides a structure parsing API based on the <code>dissect.cstruct</code> library:</p> <pre><code>class StructHandler(Handler):\n    C_DEFINITIONS: str\n    HEADER_STRUCT: str\n\n    def __init__(self):\n        self._struct_parser = StructParser(self.C_DEFINITIONS)\n\n    @property\n    def cparser_le(self):\n        return self._struct_parser.cparser_le\n\n    @property\n    def cparser_be(self):\n        return self._struct_parser.cparser_be\n\n    def parse_header(self, file: io.BufferedIOBase, endian=Endian.LITTLE):\n        header = self._struct_parser.parse(self.HEADER_STRUCT, file, endian)\n        logger.debug(\"Header parsed\", header=header, _verbosity=3)\n        return header\n</code></pre> <p>This class defines new attributes and methods:</p> <ul> <li> <p><code>C_DEFINITIONS</code>: a string holding one or multiple structures definitions in C,   which will be used to parse the format. We use the following standard to define structs:</p> <pre><code>typedef struct my_struct {\n    uint8 header_length;\n} my_struct_t;\n</code></pre> </li> <li> <p><code>HEADER_STRUCT</code>: the name of your C structure that you'll use to parse the format header.</p> </li> <li><code>parse_header()</code>: it will parse the file from the current offset in <code>endian</code>   endianness into a structure using <code>HEADER_STRUCT</code> defined in <code>C_DEFINITIONS</code>.</li> </ul> <p>If you need to parse structure using different endianness, the class exposes two properties:</p> <ul> <li><code>cparser_le</code>: <code>dissect.cstruct</code> parser configured in little endian</li> <li><code>cparser_be</code>: <code>dissect.cstruct</code> parser configured in big endian</li> </ul> <p>Recommendation</p> <p>If your format allows it, we strongly recommend you to inherit from the StructHandler given that it will be strongly typed and less prone to errors.</p>"},{"location":"development/#directoryhandler-class","title":"DirectoryHandler class","text":"<p><code>DirectoryHandler</code> is a specialized handler responsible for identifying multi-file formats located in a directory or in a subtree. The abstract class is located in unblob/models.py:</p> <pre><code>class DirectoryHandler(abc.ABC):\n    \"\"\"A directory type handler is responsible for searching, validating and \"unblobbing\" files from multiple files in a directory.\"\"\"\n\n    NAME: str\n\n    EXTRACTOR: DirectoryExtractor\n\n    PATTERN: DirectoryPattern\n\n    @classmethod\n    def get_dependencies(cls):\n        \"\"\"Return external command dependencies needed for this handler to work.\"\"\"\n        if cls.EXTRACTOR:\n            return cls.EXTRACTOR.get_dependencies()\n        return []\n\n    @abc.abstractmethod\n    def calculate_multifile(self, file: Path) -&gt; Optional[MultiFile]:\n        \"\"\"Calculate the MultiFile in a directory, using a file matched by the pattern as a starting point.\"\"\"\n\n    def extract(self, paths: List[Path], outdir: Path):\n        if self.EXTRACTOR is None:\n            logger.debug(\"Skipping file: no extractor.\", paths=paths)\n            raise ExtractError\n\n        # We only extract every blob once, it's a mistake to extract the same blob again\n        outdir.mkdir(parents=True, exist_ok=False)\n\n        self.EXTRACTOR.extract(paths, outdir)\n</code></pre> <ul> <li><code>NAME</code>: a unique name for this handler</li> <li><code>PATTERN</code>: A <code>DirectoryPattern</code> used to identify a starting/main file of the given format.</li> <li><code>EXTRACTOR</code>: a DirectoryExtractor.</li> <li><code>get_dependencies()</code>: returns the extractor dependencies. This helps unblob keep   track of third party dependencies.</li> <li><code>calculate_multifile()</code>: this is the method that needs to be overridden in your   handler. It receives a <code>file</code> Path object identified by the <code>PATTERN</code> in the directory.   This is where you implement the logic to compute and return the <code>MultiFile</code> file set.</li> </ul> <p>Any files that are being processed as part of a <code>MultiFile</code> set would be skipped from <code>Chunk</code> detection.</p> <p>Any file that is part of multiple <code>MultiFile</code> is a collision and results in a processing error.</p>"},{"location":"development/#example-handler-implementation","title":"Example Handler implementation","text":"<p>Let's imagine that we have a custom file format that always starts with the magic: <code>UNBLOB!!</code>, followed by the size of the file (header included) as an unsigned 32 bit integer.</p> <p>First, we create a file in <code>unblob/handlers/archive/myformat.py</code> and write the skeleton of our handler:</p> <pre><code>class MyformatHandler(StructHandler):\n    NAME = \"myformat\"\n\n    PATTERNS = []\n    C_DEFINITIONS = \"\"\n    HEADER_STRUCT = \"\"\n    EXTRACTOR = None\n\n    def calculate_chunk(self, file: io.BufferedIOBase, start_offset: int) -&gt; Optional[ValidChunk]:\n        return\n</code></pre> <p>We need to match on our custom magic. To find the right offset, we need to match on the <code>UNBLOB!!</code> byte pattern, so we add a <code>HexString</code> Hyperscan rule:</p> <pre><code>class MyformatHandler(StructHandler):\n    NAME = \"myformat\"\n\n    PATTERNS = [\n        HexString(\"55 4E 42 4C 4F 42 21 21\"),  # \"UNBLOB!!\"\n    ]\n\n    C_DEFINITIONS = \"\"\n    HEADER_STRUCT = \"\"\n    EXTRACTOR = None\n\n    def calculate_chunk(self, file: io.BufferedIOBase, start_offset: int) -&gt; Optional[ValidChunk]:\n        return\n</code></pre> <p>Then we need to parse the header, so we define a C structure in <code>C_DEFINITIONS</code> and adapt <code>HEADER_STRUCT</code> accordingly:</p> <pre><code>class MyformatHandler(StructHandler):\n    NAME = \"myformat\"\n\n    PATTERNS = [\n        HexString(\"55 4E 42 4C 4F 42 21 21\"),  # \"UNBLOB!!\"\n    ]\n\n    C_DEFINITIONS= r\"\"\"\n        typedef struct myformat_header {\n            char magic[8];\n            uint32 size;\n        } myformat_header_t;\n    \"\"\"\n    HEADER_STRUCT= \"myformat_header_t\"\n\n    EXTRACTOR = None\n\n    def calculate_chunk(self, file: io.BufferedIOBase, start_offset: int) -&gt; Optional[ValidChunk]:\n        return\n</code></pre> <p>With everything set, all that is left is to implement the <code>calculate_chunk</code> function:</p> <pre><code>class MyformatHandler(StructHandler):\n    NAME = \"myformat\"\n\n    PATTERNS = [\n        HexString(\"55 4E 42 4C 4F 42 21 21\"),  # \"UNBLOB!!\"\n    ]\n\n    C_DEFINITIONS= r\"\"\"\n        typedef struct myformat_header {\n            char magic[8];\n            uint32 size;\n        } myformat_header_t;\n    \"\"\"\n    HEADER_STRUCT= \"myformat_header_t\"\n\n    EXTRACTOR = None\n\n    def calculate_chunk(self, file: io.BufferedIOBase, start_offset: int) -&gt; Optional[ValidChunk]:\n        header = self.parse_header(file, Endian.LITTLE)\n        end_offset = start_offset + header.size\n        return ValidChunk(start_offset=start_offset, end_offset=end_offset)\n</code></pre> <p>That's it! Now you have a working handler for your own custom format!</p>"},{"location":"development/#testing-handlers","title":"Testing Handlers","text":"<p>If you want to submit a new format handler to unblob, it needs to come up with its own set of integration tests.</p> <p>We've implemented integration tests this way:</p> <ol> <li>pytest picks up integration test files corresponding to your handler in    <code>test/integration/type/handler_name/__input__</code> directory.</li> <li>pytest runs unblob on all the integration test files it picked up in the first step.</li> <li>pytest runs <code>diff</code> between the temporary extraction directory and    <code>test/integration/type/handler_name/__output__</code>.</li> <li>if no differences are observed the test pass, otherwise it fails.</li> </ol> <p>Important</p> <p>Create integration test files that cover all the possible scenarios of the target format.</p> <p>That includes different endianness, different versions, different padding, different algorithms. An excellent example of this is the integration test files for JFFS2 filesystems where we have filesystems covering both endianness (big endian, little endian), with or without padding, and with different compression algorithms (no compression, zlib, rtime, lzo):</p> <pre><code>./fruits.new.be.zlib.padded.jffs2\n./fruits.new.be.nocomp.padded.jffs2\n./fruits.new.be.rtime.jffs2\n./fruits.new.le.lzo.jffs2\n./fruits.new.le.rtime.jffs2\n./fruits.new.le.nocomp.padded.jffs2\n./fruits.new.be.rtime.padded.jffs2\n./fruits.new.be.lzo.jffs2\n./fruits.new.be.zlib.jffs2\n./fruits.new.le.zlib.padded.jffs2\n./fruits.new.be.lzo.padded.jffs2\n./fruits.new.le.lzo.padded.jffs2\n./fruits.new.be.nocomp.jffs2\n./fruits.new.le.zlib.jffs2\n./fruits.new.le.rtime.padded.jffs2\n./fruits.new.le.nocomp.jffs2\n</code></pre>"},{"location":"development/#importing-handlers-through-plugins","title":"Importing handlers through plugins","text":"<p>unblob uses pluggy to provide a plugin interface for users. As a developer, this interface streamlines the process of implementing and distributing new handlers and extractors.</p> <p>To export a new handler via the plugin management system, you must add a hook for either <code>unblob_register_handlers</code> or <code>unblob_register_dir_handlers</code>:</p> <ul> <li>Create a hook for <code>unblob_register_dir_handlers</code> if you are creating a new   <code>DirectoryHandler</code> implementation.</li> <li>Otherwise, register a hook for the <code>unblob_register_handlers</code> function.</li> </ul> <p>Let's consider our earlier <code>MyformatHandler</code> example. Suppose we wished to implement this handler as a plugin in a directory called <code>myplugins/</code>. Then we would create a file <code>myplugins/myformat.py</code> with the following content:</p> <pre><code>from unblob.models import Handler, StructHandler\nfrom unblob.plugins import hookimpl\n\nclass MyformatHandler(StructHandler):\n    # Format-handling code would go here\n    ...\n\n@hookimpl\ndef unblob_register_handlers() -&gt; list[type[Handler]]:\n    return [MyformatHandler]\n</code></pre> <p>Since <code>MyformatHandler</code> is a <code>StructHandler</code>, we hook the <code>unblob_register_handlers</code> function. We then return a list of all of the handler classes that we wish to register.</p> <p>To use this plugin, we would run unblob with the <code>-P</code> / <code>--plugins-path</code> pointing to the directory containing our plugin, e.g.</p> <pre><code>unblob -P ./myplugins/ ...\n</code></pre>"},{"location":"development/#utilities-functions","title":"Utilities Functions","text":"<p>We developed a bunch of utility functions which helped us during the development of existing unblob handlers. Do not hesitate to take a look at them in unblob/file_utils.py to see if any of those functions could help you during your own handler development.</p>"},{"location":"development/#hyperscan-rules","title":"Hyperscan Rules","text":"<p>Our hyperscan-based implementation accepts two different kinds of rule definitions: <code>Regex</code> and <code>HexString</code>.</p>"},{"location":"development/#regex","title":"Regex","text":"<p>This object simply represents any regular expression. Example:</p> <pre><code>PATTERNS = [\n    Regex(r\"-lh0-\")\n]\n</code></pre>"},{"location":"development/#hexstring","title":"HexString","text":"<p>This object can be used to write rules using the same DSL as Yara. The only limitation is that we do not support multi-line comments and unbounded jumps. Here's an example of a Hyperscan rule based on <code>HexString</code>:</p> <pre><code>PATTERNS = [\n    HexString(\"\"\"\n        // this is a comment\n        AA 00 [2] 01\n    \"\"\")\n]\n</code></pre> <p>In addition, start and end of input anchors (<code>^</code> and <code>$</code> like in regular expressions) can also be used to restrict a match to the beginning or the end of the input file.</p>"},{"location":"development/#directorypatterns","title":"DirectoryPatterns","text":"<p>The <code>DirectoryHandler</code> uses these patterns to identify the starting/main file of a given multi-file format. There are currently two main types: <code>Glob</code> and <code>SingleFile</code></p>"},{"location":"development/#glob","title":"Glob","text":"<p>The <code>Glob</code> object can use traditional globbing to detect files in a directory. This could be used when the file could have a varying part. There are cases where multiple multi-file set could be in a single directory. The job of the <code>DirectoryPattern</code> is to recognize the main file for each set.</p> <p>Here is an example on <code>Glob</code>:</p> <pre><code>PATTERN = Glob(\"*.7z.001\")\n</code></pre> <p>This example identify the first volume of a multi-volume sevenzip archive. Notice that this could pick up all first volumes in a given directory. (NB: Detecting the other volumes of a given set is the responsibility of the <code>DirectoryHandler.calculate_multifile</code> function. Do not write a <code>Glob</code> which picks up all the files of a multi-file set as that would result in errors.)</p>"},{"location":"development/#singlefile","title":"SingleFile","text":"<p>The <code>SingleFile</code> object can be used to identify a single file with a known name. (Obviously only use this if the main file name is well-known and does not have a varying part. It also means that only a single multi-file set can be detected in a given directory.)</p> <p>Here is an example on <code>SingleFile</code>:</p> <pre><code>PATTERN = SingleFile(\"meta-data.json\")\n</code></pre> <p>This would pick up the file <code>meta-data.json</code> and pass it to the <code>DirectoryHandler</code>. The handler still has to verify the file and has to find the additional files.</p>"},{"location":"development/#writing-extractors","title":"Writing extractors","text":"<p>Recommendation</p> <p>We support custom Python based extractors as part of unblob, but unless you write a handler for an exotic format, you should check if the Command extractor is sufficient for your needs, as it's very simple to use.</p>"},{"location":"development/#command-extractor","title":"Command extractor","text":"<p>This extractor simply runs a command line tool on the carved-out file (<code>inpath</code>) to extract into the extraction directory (<code>outdir</code>). Below is the <code>Command</code> extractor instance of the ZIP handler:</p> <pre><code>EXTRACTOR = Command(\"7z\", \"x\", \"-p\", \"-y\", \"{inpath}\", \"-o{outdir}\")\n</code></pre> <p>If you have a custom format with no supported command to extract it, check out the <code>Extractor</code> Python class.</p>"},{"location":"development/#extractor-class","title":"Extractor class","text":"<p>The <code>Extractor</code> interface is defined in unblob/models.py:</p> <pre><code>class Extractor(abc.ABC):\n    def get_dependencies(self) -&gt; List[str]:\n        \"\"\"Returns the external command dependencies.\"\"\"\n        return []\n\n    @abc.abstractmethod\n    def extract(self, inpath: Path, outdir: Path) -&gt; Optional[ExtractResult]:\n        \"\"\"Extract the carved out chunk. Raises ExtractError on failure.\"\"\"\n</code></pre> <p>Two methods are exposed by this class:</p> <ul> <li><code>get_dependencies()</code>: you should override it if your custom extractor relies on   external dependencies such as command line tools</li> <li><code>extract()</code>: you must override this function. This is where you'll perform the   extraction of <code>inpath</code> content into <code>outdir</code> extraction directory</li> </ul> <p>Recommendation</p> <p>Although it is possible to implement <code>extract()</code> with path manipulations, checks for path traversals, and performing io by using Python libraries (<code>os</code>, <code>pathlib.Path</code>), but it turns out somewhat tedious. Instead we recommend to remove boilerplate and use a helper class <code>FileSystem</code> from unblob/file_utils.py which ensures that all file objects are created under its root.</p>"},{"location":"development/#directoryextractor-class","title":"DirectoryExtractor class","text":"<p>The <code>DirectoryExtractor</code> interface is defined in unblob/models.py:</p> <pre><code>class DirectoryExtractor(abc.ABC):\n    def get_dependencies(self) -&gt; List[str]:\n        \"\"\"Return the external command dependencies.\"\"\"\n        return []\n\n    @abc.abstractmethod\n    def extract(self, paths: List[Path], outdir: Path) -&gt; Optional[ExtractResult]:\n        \"\"\"Extract from a multi file path list.\n\n        Raises ExtractError on failure.\n        \"\"\"\n</code></pre> <p>Two methods are exposed by this class:</p> <ul> <li><code>get_dependencies()</code>: you should override it if your custom extractor relies on   external dependencies such as command line tools</li> <li><code>extract()</code>: you must override this function. This is where you'll perform the   extraction of <code>paths</code> files into <code>outdir</code> extraction directory</li> </ul> <p>Recommendation</p> <p>Similarly to <code>Extractor</code>, it is recommended to use the <code>FileSystem</code> helper class to implement <code>extract</code>.</p>"},{"location":"development/#example-extractor","title":"Example Extractor","text":"<p>Extractors are quite complex beasts, so rather than trying to come up with a fake example, we recommend you to read through our RomFS extractor code to see what it looks like in real world applications.</p>"},{"location":"development/#guidelines","title":"Guidelines","text":""},{"location":"development/#code-style","title":"Code style","text":"<p>We adhere to PEP8 and enforce proper formatting of source files using ruff format so you should not worry about formatting source code at all, <code>pre-commit</code> will take care of it.</p> <p>For linting we use ruff check. Lint errors can be shown in your editor of choice by one of the editor plugins.</p>"},{"location":"development/#file-format-correctness","title":"File Format Correctness","text":"<p>We want to strike the right balance between false positive reduction and a totally loose implementation. We tend not to validate checksums in order to still be able to extract corrupted content. However, if the lack of checksum validation gets in the way by leaving the handler generating a large amount of false positive, then it's time to revisit the handler and implement stronger header checks.</p>"},{"location":"development/#common-unblob-handler-mistakes","title":"Common unblob Handler Mistakes","text":"<p>This is a collection of all the bad code we've seen during unblob development. Learn from us so you can avoid them in the future \ud83d\ude42</p> <ul> <li>Use <code>seek</code> rather than <code>read</code> whenever possible, it's faster.</li> <li>You should always keep in mind to <code>seek</code> to the position the header starts or make sure you are always at the correct   offset at all times. For example we made the mistake multiple times that read 4 bytes for file magic and didn't seek   back.</li> <li>Watch out for negative seeking</li> <li>Make sure you get your types right! signedness can get in the way.</li> <li>Try to use as specific as possible patterns to identify data in Handlers to avoid false-positive matches   and extra processing in the Handler.</li> <li>Try to avoid using overlapping patterns, as patterns that match on the same data could easily collide. Hyperscan   does not guarantee priority between patterns matching on the same data. (Hyperscan reports matches ordered by the   pattern match end offset. In case multiple pattern match on the same end offset the matching order depends on the   pattern registration order which is undefined in unblob.)</li> </ul>"},{"location":"extractors/","title":"Extractors","text":"<p>unblob relies on various tools for extracting the contents of a blob. These extractors are either third party tools (e.g. 7z), or part of unblob (available in <code>unblob/extractors</code> directory or specific ones next to the handler, e.g.: <code>unblob/handlers/filesystem/romfs.py</code>).</p> <p>To use unblob with all supported formats, all extractors need to be installed.</p> <p>See the Installation section on how to install extractors for various methods. You don't need to install any of these if you use Docker or Nix, as all extractors are included in those solutions.</p>"},{"location":"extractors/#checking-installed-extractors","title":"Checking installed extractors","text":"<p>There is a <code>--show-external-dependencies</code> CLI option, which displays the name of the extractors used by unblob and shows if they are available for unblob to use or not:</p> <pre><code>$ unblob --show-external-dependencies\nThe following executables found installed, which are needed by unblob:\n    7z                          \u2713\n    debugfs                     \u2713\n    jefferson                   \u2713\n    lz4                         \u2713\n    lziprecover                 \u2713\n    lzop                        \u2713\n    sasquatch                   \u2713\n    sasquatch-v4be              \u2713\n    simg2img                    \u2713\n    ubireader_extract_files     \u2713\n    ubireader_extract_images    \u2713\n    unar                        \u2713\n    zstd                        \u2713\n</code></pre> <p>NOTE: This option does NOT check the version of the extractors.</p>"},{"location":"extractors/#required-extractors","title":"Required extractors","text":"<p>\u274c: If you installed unblob from source, you need to install these manually.</p> <p>\u2705: These extractors come with unblob, check pyproject.toml and uv.lock for current versions.</p> Extractor Provided commands Minimum version Pre-Installed More information p7zip-full <code>7z</code> 16.02 \u274c https://www.7-zip.org/ e2fsprogs <code>debugfs</code> 1.45.5 \u274c http://e2fsprogs.sourceforge.net/ lz4 <code>lz4</code> 1.9.3 \u274c https://github.com/lz4/lz4 lziprecover <code>lziprecover</code> 1.22 \u274c http://www.nongnu.org/lzip/lziprecover.html lzop <code>lzop</code> 1.04 \u274c https://www.lzop.org/ android-sdk-libsparse-utils <code>img2simg</code> 8.1.0 \u274c https://packages.debian.org/unstable/android-sdk-libsparse-utils unar <code>unar</code> 1.10.1 \u274c https://theunarchiver.com/command-line sasquatch <code>sasquatch</code>, <code>sasquatch-v4be</code> 1.0 \u274c https://github.com/onekey-sec/sasquatch jefferson <code>jefferson</code> master \u2705 https://github.com/onekey-sec/jefferson ubireader <code>ubireader_extract_files</code>, <code>ubireader_extract_images</code> master \u2705 https://github.com/onekey-sec/ubi_reader"},{"location":"extractors/#maintained-projects-and-forks","title":"Maintained projects and forks","text":"<p>We maintain a fork of several extractors, with many fixes and improvements. They are also available on GitHub:</p> <ul> <li>Jefferson for extracting JFFS2 is also a project of ONEKEY</li> <li>Fork of sasquatch based on squashfs-tools</li> <li>Fork of ubi_reader Python scripts for extracting UBI and UBIFS images</li> </ul>"},{"location":"formats/","title":"Supported Formats","text":"<p>Didn't find what you're looking for? (1)</p> <ol> <li> <p>unblob is easily extensible, and you can write your own handler and include your own extractors for proprietary formats.      To learn more about this, see the development section.      Alternatively, just open a new ticket in the Github issue tracker.</p> <p>Whenever we stumble upon proprietary formats in our ONEKEY analysis platform, we will add support for it.  At this point, we have developed about a dozen of additional, proprietary format Handlers.</p> <p>If you are interested in a custom format not supported by the open source version, check out our platform at  https://www.onekey.com or you can Contact Us.</p> </li> </ol> All supported formats Format Type Fully supported? <code>7-ZIP</code> ARCHIVE <code>ANDROID EROFS</code> FILESYSTEM <code>ANDROID SPARSE</code> FILESYSTEM <code>AR</code> ARCHIVE <code>ARC</code> ARCHIVE <code>ARJ</code> ARCHIVE <code>AUTEL ECC</code> ARCHIVE <code>BZIP2</code> COMPRESSION <code>CAB</code> ARCHIVE <code>COMPRESS</code> COMPRESSION <code>CPIO (BINARY)</code> ARCHIVE <code>CPIO (PORTABLE ASCII CRC)</code> ARCHIVE <code>CPIO (PORTABLE ASCII)</code> ARCHIVE <code>CPIO (PORTABLE OLD ASCII)</code> ARCHIVE <code>CRAMFS</code> FILESYSTEM <code>D-LINK ENCRPTED_IMG</code> ARCHIVE <code>D-LINK SHRS</code> ARCHIVE <code>DMG</code> ARCHIVE <code>ELF (32-BIT)</code> EXECUTABLE <code>ELF (64-BIT)</code> EXECUTABLE <code>ENGENIUS</code> ARCHIVE <code>EXTFS</code> FILESYSTEM <code>FAT</code> FILESYSTEM <code>GZIP</code> COMPRESSION <code>GZIP (MULTI-VOLUME)</code> COMPRESSION <code>HP BDL</code> ARCHIVE <code>HP IPKG</code> ARCHIVE <code>INSTAR BNEG</code> ARCHIVE <code>INSTAR HD</code> ARCHIVE <code>ISO 9660</code> FILESYSTEM <code>JFFS2 (NEW)</code> FILESYSTEM <code>JFFS2 (OLD)</code> FILESYSTEM <code>LZ4</code> COMPRESSION <code>LZ4 (LEGACY)</code> COMPRESSION <code>LZ4 (SKIPPABLE)</code> COMPRESSION <code>LZH</code> COMPRESSION <code>LZIP</code> COMPRESSION <code>LZMA</code> COMPRESSION <code>LZO</code> COMPRESSION <code>MULTI-SEVENZIP</code> ARCHIVE <code>NETGEAR CHK</code> ARCHIVE <code>NETGEAR TRX V1</code> ARCHIVE <code>NETGEAR TRX V2</code> ARCHIVE <code>NTFS</code> FILESYSTEM <code>PARTCLONE</code> ARCHIVE <code>QNAP NAS</code> ARCHIVE <code>RAR</code> ARCHIVE <code>ROMFS</code> FILESYSTEM <code>SQUASHFS (V1)</code> FILESYSTEM <code>SQUASHFS (V2)</code> FILESYSTEM <code>SQUASHFS (V3)</code> FILESYSTEM <code>SQUASHFS (V3-BROADCOM)</code> FILESYSTEM <code>SQUASHFS (V3-DDWRT)</code> FILESYSTEM <code>SQUASHFS (V3-NON-STANDARD)</code> FILESYSTEM <code>SQUASHFS (V4-BE)</code> FILESYSTEM <code>SQUASHFS (V4-LE)</code> FILESYSTEM <code>STUFFIT SIT</code> ARCHIVE <code>STUFFIT SIT (V5)</code> ARCHIVE <code>TAR (UNIX)</code> ARCHIVE <code>TAR (USTAR)</code> ARCHIVE <code>UBI</code> FILESYSTEM <code>UBIFS</code> FILESYSTEM <code>UZIP</code> COMPRESSION <code>XIAOMI HDR1</code> ARCHIVE <code>XIAOMI HDR2</code> ARCHIVE <code>XZ</code> COMPRESSION <code>YAFFS</code> FILESYSTEM <code>ZIP</code> ARCHIVE <code>ZLIB</code> COMPRESSION <code>ZSTD</code> COMPRESSION"},{"location":"formats/#7-zip","title":"7-Zip","text":"<p>Fully supported</p> DescriptionReferences <p>The 7-Zip file format is a compressed archive format with high compression ratios, supporting multiple algorithms, CRC checks, and multi-volume archives.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>7-Zip Technical Documentation</li> </ul>"},{"location":"formats/#android-erofs","title":"Android EROFS","text":"<p>Fully supported</p> DescriptionReferences <p>EROFS (Enhanced Read-Only File System) is a lightweight, high-performance file system designed for read-only use cases, commonly used in Android and Linux. It features compression support, metadata efficiency, and a fixed superblock structure.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: Google</li> </ul> <ul> <li>EROFS Documentation</li> <li>EROFS Wikipedia</li> </ul>"},{"location":"formats/#android-sparse","title":"Android Sparse","text":"<p>Fully supported</p> DescriptionReferences <p>Android sparse images are a file format used to efficiently store disk images by representing sequences of zero blocks compactly. The format includes a file header, followed by chunk headers and data, with support for raw, fill, and 'don't care' chunks.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: Google</li> </ul> <ul> <li>Android Sparse Image Format Documentation</li> <li>simg2img Tool</li> </ul>"},{"location":"formats/#ar","title":"AR","text":"<p>Fully supported</p> DescriptionReferences <p>Unix AR (archive) files are used to store multiple files in a single archive with a simple header format.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>Unix AR File Format Documentation</li> </ul>"},{"location":"formats/#arc","title":"ARC","text":"<p>Fully supported</p> DescriptionReferences <p>ARC is a legacy archive format used to store multiple files with metadata such as file size, creation date, and CRC.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>ARC File Format Documentation</li> </ul>"},{"location":"formats/#arj","title":"ARJ","text":"<p>Fully supported</p> DescriptionReferences <p>ARJ is a legacy compressed archive formats used to store multiple files with metadata such as file size, creation date, and CRC.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>ARJ File Format Documentation</li> <li>ARJ Technical Information</li> </ul>"},{"location":"formats/#autel-ecc","title":"Autel ECC","text":"<p>Fully supported</p> DescriptionReferences <p>Autel ECC files consist of a custom header followed by encrypted data blocks. The header includes metadata such as magic bytes, file size, and copyright information.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Autel</li> </ul> <ul> <li>Autel ECC Decryption Script (Sector7)</li> </ul>"},{"location":"formats/#bzip2","title":"bzip2","text":"<p>Fully supported</p> DescriptionReferences <p>The bzip2 format is a block-based compression format that uses the Burrows-Wheeler transform and Huffman coding for high compression efficiency. Each stream starts with a header and consists of one or more compressed blocks, ending with a footer containing a checksum.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>bzip2 File Format Documentation</li> <li>bzip2 Technical Specification</li> </ul>"},{"location":"formats/#cab","title":"CAB","text":"<p>Fully supported</p> DescriptionReferences <p>Microsoft Cabinet (CAB) archive files are used for compressed file storage and software installation.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Microsoft</li> </ul> <ul> <li>Microsoft Cabinet File Format Documentation</li> <li>Ubuntu Manual - cabextract</li> </ul>"},{"location":"formats/#compress","title":"compress","text":"<p>Fully supported</p> DescriptionReferences <p>Unix compress files use the Lempel-Ziv-Welch (LZW) algorithm for data compression and are identified by a 2-byte magic number (0x1F 0x9D). This format supports optional block compression and variable bit lengths ranging from 9 to 16 bits.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>Unix Compress File Format Documentation</li> <li>LZW Compression Algorithm</li> </ul>"},{"location":"formats/#cpio-binary","title":"CPIO (binary)","text":"<p>Fully supported</p> DescriptionReferences <p>CPIO (Copy In, Copy Out) is an archive file format used for bundling files and directories along with their metadata. It is commonly used in Unix-like systems for creating backups or transferring files, and supports various encoding formats including binary and ASCII.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>GNU CPIO Manual</li> </ul>"},{"location":"formats/#cpio-portable-ascii-crc","title":"CPIO (portable ASCII CRC)","text":"<p>Fully supported</p> DescriptionReferences <p>CPIO (Copy In, Copy Out) is an archive file format used for bundling files and directories along with their metadata. It is commonly used in Unix-like systems for creating backups or transferring files, and supports various encoding formats including binary and ASCII.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>GNU CPIO Manual</li> </ul>"},{"location":"formats/#cpio-portable-ascii","title":"CPIO (portable ASCII)","text":"<p>Fully supported</p> DescriptionReferences <p>CPIO (Copy In, Copy Out) is an archive file format used for bundling files and directories along with their metadata. It is commonly used in Unix-like systems for creating backups or transferring files, and supports various encoding formats including binary and ASCII.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>GNU CPIO Manual</li> </ul>"},{"location":"formats/#cpio-portable-old-ascii","title":"CPIO (portable old ASCII)","text":"<p>Fully supported</p> DescriptionReferences <p>CPIO (Copy In, Copy Out) is an archive file format used for bundling files and directories along with their metadata. It is commonly used in Unix-like systems for creating backups or transferring files, and supports various encoding formats including binary and ASCII.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>GNU CPIO Manual</li> </ul>"},{"location":"formats/#cramfs","title":"CramFS","text":"<p>Fully supported</p> DescriptionReferences <p>CramFS is a lightweight, read-only file system format designed for simplicity and efficiency in embedded systems. It uses zlib compression for file data and stores metadata in a compact, contiguous structure.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>CramFS Documentation</li> <li>CramFS Wikipedia</li> </ul>"},{"location":"formats/#d-link-encrpted_img","title":"D-Link encrpted_img","text":"<p>Fully supported</p> DescriptionReferences <p>A binary format used by D-Link to store encrypted firmware or data. It consists of a custom 12-byte magic header followed by the encrypted payload.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: D-Link</li> </ul> <ul> <li>How-To: Extracting Decryption Keys for D-Link</li> </ul>"},{"location":"formats/#d-link-shrs","title":"D-Link SHRS","text":"<p>Fully supported</p> DescriptionReferences <p>SHRS is a D-Link firmware format with a custom header containing metadata, SHA-512 digests, and AES-CBC encryption parameters. The firmware data is encrypted using a fixed key and IV stored in the header.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: D-Link</li> </ul> <ul> <li>Breaking the D-Link DIR3060 Firmware Encryption - Recon - Part 1</li> </ul>"},{"location":"formats/#dmg","title":"DMG","text":"<p>Fully supported</p> DescriptionReferences <p>Apple Disk Image (DMG) files are commonly used on macOS for software distribution and disk image storage.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Apple</li> </ul> <ul> <li>Apple Disk Image Format Documentation</li> </ul>"},{"location":"formats/#elf-32-bit","title":"ELF (32-bit)","text":"<p>Fully supported</p> DescriptionReferences <p>The 32-bit ELF (Executable and Linkable Format) is a binary file format used for executables, object code, shared libraries, and core dumps. It supports 32-bit addressing and includes headers for program and section information.</p> <ul> <li>Handler type: Executable</li> </ul> <ul> <li>ELF File Format Specification</li> <li>ELF Wikipedia</li> </ul>"},{"location":"formats/#elf-64-bit","title":"ELF (64-bit)","text":"<p>Fully supported</p> DescriptionReferences <p>The 64-bit ELF (Executable and Linkable Format) is a binary file format used for executables, object code, shared libraries, and core dumps. It supports 64-bit addressing and includes headers for program and section information.</p> <ul> <li>Handler type: Executable</li> </ul> <ul> <li>ELF File Format Specification</li> <li>ELF Wikipedia</li> </ul>"},{"location":"formats/#engenius","title":"Engenius","text":"<p>Partially supported</p> DescriptionReferencesLimitations <p>Engenius firmware files contain a custom header with metadata, followed by encrypted data using an XOR cipher.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Engenius</li> </ul> <ul> <li>enfringement - Tools for working with EnGenius WiFi hardware.</li> </ul> <ul> <li>Does not support all firmware versions.</li> </ul>"},{"location":"formats/#extfs","title":"ExtFS","text":"<p>Fully supported</p> DescriptionReferences <p>ExtFS (Ext2/Ext3/Ext4) is a family of journaling file systems commonly used in Linux-based operating systems. It supports features like large file sizes, extended attributes, and journaling for improved reliability.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>Ext4 Documentation</li> <li>ExtFS Wikipedia</li> </ul>"},{"location":"formats/#fat","title":"FAT","text":"<p>Fully supported</p> DescriptionReferences <p>FAT (File Allocation Table) is a file system format used for organizing and managing files on storage devices, supporting FAT12, FAT16, and FAT32 variants. It uses a table to map file clusters, enabling efficient file storage and retrieval.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>FAT Wikipedia</li> </ul>"},{"location":"formats/#gzip","title":"GZIP","text":"<p>Fully supported</p> DescriptionReferences <p>GZIP is a compressed file format that uses the DEFLATE algorithm and includes metadata such as original file name and modification time. It is commonly used for efficient file storage and transfer.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>GZIP File Format Specification</li> <li>GZIP Wikipedia</li> </ul>"},{"location":"formats/#gzip-multi-volume","title":"GZIP (multi-volume)","text":"<p>Fully supported</p> DescriptionReferences <p>GZIP is a compressed file format that uses the DEFLATE algorithm and includes metadata such as original file name and modification time. It is commonly used for efficient file storage and transfer.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>GZIP File Format Specification</li> <li>GZIP Wikipedia</li> </ul>"},{"location":"formats/#hp-bdl","title":"HP BDL","text":"<p>Fully supported</p> DescriptionReferences <p>The HP BDL format is a firmware archive containing a custom header and a table of contents that specifies offsets and sizes of embedded firmware components. It includes metadata such as release, brand, device ID, version, and revision.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: HP</li> </ul> <ul> <li>hpbdl</li> </ul>"},{"location":"formats/#hp-ipkg","title":"HP IPKG","text":"<p>Fully supported</p> DescriptionReferences <p>HP IPKG firmware archives consist of a custom header, followed by a table of contents and file entries. Each entry specifies metadata such as file name, offset, size, and CRC32 checksum.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: HP</li> </ul> <ul> <li>hpbdl</li> </ul>"},{"location":"formats/#instar-bneg","title":"Instar BNEG","text":"<p>Fully supported</p> DescriptionReferences <p>BNEG firmware files consist of a custom header followed by two partitions containing firmware components. The header specifies metadata such as magic value, version, and partition sizes.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Instar</li> </ul>"},{"location":"formats/#instar-hd","title":"Instar HD","text":"<p>Fully supported</p> DescriptionReferences <p>Instar HD firmware files are modified ZIP archives with non-standard local file headers, central directory headers, and end-of-central-directory records. These modifications include custom magic bytes to differentiate them from standard ZIP files.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Instar</li> </ul>"},{"location":"formats/#iso-9660","title":"ISO 9660","text":"<p>Fully supported</p> DescriptionReferences <p>ISO 9660 is a file system standard for optical disc media, defining a volume descriptor structure and directory hierarchy. It is widely used for CD-ROMs and supports cross-platform compatibility.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>ISO 9660 Specification</li> <li>ISO 9660 Wikipedia</li> </ul>"},{"location":"formats/#jffs2-new","title":"JFFS2 (new)","text":"<p>Fully supported</p> DescriptionReferences <p>JFFS2 (Journaling Flash File System version 2) is a log-structured file system for flash memory devices, using an older magic number to identify its nodes. It organizes data into nodes with headers containing metadata and CRC checks for integrity.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>JFFS2 Documentation</li> <li>JFFS2 Wikipedia</li> </ul>"},{"location":"formats/#jffs2-old","title":"JFFS2 (old)","text":"<p>Fully supported</p> DescriptionReferences <p>JFFS2 (Journaling Flash File System version 2) is a log-structured file system for flash memory devices, using an older magic number to identify its nodes. It organizes data into nodes with headers containing metadata and CRC checks for integrity.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>JFFS2 Documentation</li> <li>JFFS2 Wikipedia</li> </ul>"},{"location":"formats/#lz4","title":"LZ4","text":"<p>Fully supported</p> DescriptionReferences <p>LZ4 is a high-speed lossless compression algorithm designed for real-time data compression with minimal memory usage.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZ4 Frame Format Documentation</li> <li>LZ4 Wikipedia</li> </ul>"},{"location":"formats/#lz4-legacy","title":"LZ4 (legacy)","text":"<p>Fully supported</p> DescriptionReferences <p>LZ4 legacy format is an older framing format used prior to the LZ4 Frame specification, featuring a simpler structure and no support for skippable frames or extensive metadata. Unlike the default LZ4 Frame format, it lacks built-in checksums, versioning, or block independence flags, making it less robust and primarily used for backward compatibility.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZ4 Frame Format Documentation</li> <li>LZ4 Wikipedia</li> </ul>"},{"location":"formats/#lz4-skippable","title":"LZ4 (skippable)","text":"<p>Fully supported</p> DescriptionReferences <p>LZ4 skippable format is designed to encapsulate arbitrary data within an LZ4 stream allowing compliant parsers to skip over it safely. This format does not contain compressed data itself but is often used for embedding metadata or non-LZ4 content alongside standard frames.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZ4 Frame Format Documentation</li> <li>LZ4 Wikipedia</li> </ul>"},{"location":"formats/#lzh","title":"LZH","text":"<p>Fully supported</p> DescriptionReferences <p>LZH is a legacy archive format that uses various compression methods such as '-lh0-' and '-lh5-'. It was widely used in Japan and on older systems for compressing and archiving files.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZH Compression Format</li> </ul>"},{"location":"formats/#lzip","title":"Lzip","text":"<p>Fully supported</p> DescriptionReferences <p>Lzip is a lossless compressed file format based on the LZMA algorithm. It features a simple header, CRC-checked integrity, and efficient compression for large files.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>Lzip File Format Documentation</li> <li>Lzip Wikipedia</li> </ul>"},{"location":"formats/#lzma","title":"LZMA","text":"<p>Fully supported</p> DescriptionReferences <p>LZMA is a compression format based on the Lempel-Ziv-Markov chain algorithm, offering high compression ratios and efficient decompression. It is commonly used in standalone <code>.lzma</code> files and embedded in other formats like 7z.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZMA File Format Documentation</li> <li>LZMA Wikipedia</li> </ul>"},{"location":"formats/#lzo","title":"LZO","text":"<p>Fully supported</p> DescriptionReferences <p>LZO is a data compression format featuring a simple header structure and optional checksum verification. It is optimized for fast decompression and supports various compression levels and flags for additional metadata.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZO File Format Documentation</li> <li>LZO Wikipedia</li> </ul>"},{"location":"formats/#multi-sevenzip","title":"multi-sevenzip","text":"<p>Fully supported</p> DescriptionReferences <p>The 7-Zip file format is a compressed archive format with high compression ratios, supporting multiple algorithms, CRC checks, and multi-volume archives.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>7-Zip Technical Documentation</li> </ul>"},{"location":"formats/#netgear-chk","title":"Netgear CHK","text":"<p>Fully supported</p> DescriptionReferences <p>Netgear CHK firmware files consist of a custom header containing metadata and checksums, followed by kernel and root filesystem partitions. The header includes fields for partition sizes, checksums, and a board identifier.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Netgear</li> </ul> <ul> <li>CHK Image Format Image Builder Tool for the R7800 Series</li> </ul>"},{"location":"formats/#netgear-trx-v1","title":"Netgear TRX v1","text":"<p>Fully supported</p> DescriptionReferences <p>Netgear TRX v1 firmware format includes a custom header with partition offsets and a CRC32 checksum for integrity verification. It supports up to three partitions defined in the header.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Netgear</li> </ul>"},{"location":"formats/#netgear-trx-v2","title":"Netgear TRX v2","text":"<p>Fully supported</p> DescriptionReferences <p>Netgear TRX v2 firmware format includes a custom header with partition offsets and a CRC32 checksum for integrity verification. It supports up to four partitions defined in the header.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Netgear</li> </ul>"},{"location":"formats/#ntfs","title":"NTFS","text":"<p>Fully supported</p> DescriptionReferences <p>NTFS (New Technology File System) is a proprietary file system developed by Microsoft, featuring metadata support, advanced data structures, and journaling for reliability. It is commonly used in Windows operating systems for efficient storage and retrieval of files.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: Microsoft</li> </ul> <ul> <li>NTFS Wikipedia</li> </ul>"},{"location":"formats/#partclone","title":"Partclone","text":"<p>Fully supported</p> DescriptionReferences <p>Partclone is a utility used for backing up and restoring partitions. Many cloning tools (such as Clonezilla) rely on it to create block-level images that include filesystem metadata.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>Partclone GitHub Repository</li> <li>Clonezilla Official Documentation</li> </ul>"},{"location":"formats/#qnap-nas","title":"QNAP NAS","text":"<p>Fully supported</p> DescriptionReferences <p>QNAP NAS firmware files consist of a custom header, encrypted data sections, and a footer marking the end of the encrypted stream. The header contains metadata such as device ID, firmware version, and encryption details.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: QNAP</li> </ul>"},{"location":"formats/#rar","title":"RAR","text":"<p>Partially supported</p> DescriptionReferencesLimitations <p>RAR archive files are commonly used for compressed data storage. They can contain multiple files and directories, and support various compression methods.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>RAR 4.x File Format Documentation</li> <li>RAR 5.x File Format Documentation</li> </ul> <ul> <li>Does not support encrypted RAR files.</li> </ul>"},{"location":"formats/#romfs","title":"RomFS","text":"<p>Fully supported</p> DescriptionReferences <p>RomFS is a simple, space-efficient, read-only file system format designed for embedded systems. It features 16-byte alignment, minimal metadata overhead, and supports basic file types like directories, files, symlinks, and devices.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>RomFS Documentation</li> <li>RomFS Wikipedia</li> </ul>"},{"location":"formats/#squashfs-v1","title":"SquashFS (v1)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 1 is a compressed, read-only file system format designed for minimal storage usage. It is commonly used in embedded systems and early Linux distributions.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"formats/#squashfs-v2","title":"SquashFS (v2)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 2 is a compressed, read-only file system format designed for minimal storage usage. It builds upon version 1 with additional features and improvements for embedded systems and Linux distributions.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"formats/#squashfs-v3","title":"SquashFS (v3)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 3 is a compressed, read-only file system format designed for minimal storage usage. It is widely used in embedded systems and Linux distributions for efficient storage and fast access.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"formats/#squashfs-v3-broadcom","title":"SquashFS (v3-Broadcom)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 3 Broadcom is a variant of the SquashFS v3 format used in Broadcom firmware. It features a unique magic number and may include specific optimizations for Broadcom devices.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: Broadcom</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"formats/#squashfs-v3-ddwrt","title":"SquashFS (v3-DDWRT)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 3 DD-WRT is a variant of the SquashFS v3 format used in DD-WRT firmware. It features a unique magic number and may include specific optimizations for embedded systems.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: DDWRT</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"formats/#squashfs-v3-non-standard","title":"SquashFS (v3-non-standard)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 3 is a compressed, read-only file system format designed for minimal storage usage. It is widely used in embedded systems and Linux distributions for efficient storage and fast access.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: unknown</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"formats/#squashfs-v4-be","title":"SquashFS (v4-BE)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 4 is a compressed, read-only file system format designed for minimal storage usage and fast access. It supports both big-endian and little-endian formats and is widely used in embedded systems and Linux distributions.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"formats/#squashfs-v4-le","title":"SquashFS (v4-LE)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 4 is a compressed, read-only file system format designed for minimal storage usage and fast access. It is widely used in embedded systems and Linux distributions for efficient storage management.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"formats/#stuffit-sit","title":"Stuffit SIT","text":"<p>Fully supported</p> DescriptionReferences <p>StuffIt SIT archives is a legacy compressed archive format commonly used on macOS and earlier Apple systems.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: StuffIt Technologies</li> </ul> <ul> <li>StuffIt SIT File Format Documentation</li> </ul>"},{"location":"formats/#stuffit-sit-v5","title":"Stuffit SIT (v5)","text":"<p>Fully supported</p> DescriptionReferences <p>StuffIt SIT archives is a legacy compressed archive format commonly used on macOS and earlier Apple systems.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: StuffIt Technologies</li> </ul> <ul> <li>StuffIt SIT File Format Documentation</li> </ul>"},{"location":"formats/#tar-unix","title":"TAR (Unix)","text":"<p>Fully supported</p> DescriptionReferences <p>Unix tar files are a widely used archive format for storing files and directories with metadata.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>Unix Tar Format Documentation</li> <li>GNU Tar Manual</li> </ul>"},{"location":"formats/#tar-ustar","title":"TAR (USTAR)","text":"<p>Fully supported</p> DescriptionReferences <p>USTAR (Uniform Standard Tape Archive) tar files are extensions of the original tar format with additional metadata fields.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>USTAR Format Documentation</li> <li>POSIX Tar Format Specification</li> </ul>"},{"location":"formats/#ubi","title":"UBI","text":"<p>Fully supported</p> DescriptionReferences <p>UBI (Unsorted Block Image) is a volume management system for raw flash devices, providing wear leveling and bad block management. It operates as a layer between the MTD subsystem and higher-level filesystems like UBIFS.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>UBI Documentation</li> <li>UBI Wikipedia</li> </ul>"},{"location":"formats/#ubifs","title":"UBIFS","text":"<p>Fully supported</p> DescriptionReferences <p>UBIFS (Unsorted Block Image File System) is a flash file system designed for raw flash memory, providing wear leveling, error correction, and power failure resilience. It operates on top of UBI volumes, which manage flash blocks on raw NAND or NOR flash devices.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>UBIFS Documentation</li> <li>UBIFS Wikipedia</li> </ul>"},{"location":"formats/#uzip","title":"UZIP","text":"<p>Fully supported</p> DescriptionReferences <p>FreeBSD UZIP is a block-based compressed disk image format. It uses a table of contents to index compressed blocks, supporting ZLIB, LZMA, and ZSTD compression algorithms.</p> <ul> <li>Handler type: Compression</li> <li>Vendor: FreeBSD</li> </ul> <ul> <li>FreeBSD UZIP Documentation</li> </ul>"},{"location":"formats/#xiaomi-hdr1","title":"Xiaomi HDR1","text":"<p>Fully supported</p> DescriptionReferences <p>Xiaomi HDR1 firmware files feature a custom header containing metadata, CRC32 checksum, and blob offsets for embedded data. These files are used in Xiaomi devices for firmware updates.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Xiaomi</li> </ul>"},{"location":"formats/#xiaomi-hdr2","title":"Xiaomi HDR2","text":"<p>Fully supported</p> DescriptionReferences <p>Xiaomi HDR2 firmware files feature a custom header with metadata, CRC32 checksum, and blob offsets for embedded data. These files also include additional fields for device ID and region information.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Xiaomi</li> </ul>"},{"location":"formats/#xz","title":"XZ","text":"<p>Fully supported</p> DescriptionReferences <p>XZ is a compressed file format that uses the LZMA2 algorithm for high compression efficiency. It is designed for general-purpose data compression with support for integrity checks and padding for alignment.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>XZ File Format Specification</li> <li>XZ Wikipedia</li> </ul>"},{"location":"formats/#yaffs","title":"YAFFS","text":"<p>Fully supported</p> DescriptionReferences <p>YAFFS (Yet Another Flash File System) is a log-structured file system designed for NAND flash memory, storing data in fixed-size chunks with associated metadata. It supports features like wear leveling, error correction, and efficient handling of power loss scenarios.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>YAFFS Documentation</li> <li>YAFFS Wikipedia</li> </ul>"},{"location":"formats/#zip","title":"ZIP","text":"<p>Partially supported</p> DescriptionReferencesLimitations <p>ZIP is a widely used archive file format that supports multiple compression methods, file spanning, and optional encryption. It includes metadata such as file names, sizes, and timestamps, and supports both standard and ZIP64 extensions for large files.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>ZIP File Format Specification</li> <li>ZIP64 Format Specification</li> </ul> <ul> <li>Does not support encrypted ZIP files.</li> </ul>"},{"location":"formats/#zlib","title":"zlib","text":"<p>Fully supported</p> DescriptionReferences <p>The zlib format is a compressed data format based on the DEFLATE algorithm, often used for data compression in various applications. It includes a lightweight header and checksum for data integrity.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>zlib File Format Specification</li> <li>zlib Wikipedia</li> </ul>"},{"location":"formats/#zstd","title":"ZSTD","text":"<p>Fully supported</p> DescriptionReferences <p>Zstandard (ZSTD) is a fast lossless compression algorithm with high compression ratios, designed for modern data storage and transfer. Its file format includes a frame structure with optional dictionary support and checksums for data integrity.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>Zstandard File Format Specification</li> <li>Zstandard Wikipedia</li> </ul>"},{"location":"glossary/","title":"Glossary","text":""},{"location":"glossary/#handler","title":"Handler","text":"a <code>Handler</code> in unblob is a Python class, which can detect and extract a specific file format."},{"location":"glossary/#extractor","title":"Extractor","text":"A tool, which can extract a file format, e.g. <code>7z</code>, <code>unrar</code>, <code>jefferson</code>, etc. One tool might be used for multiple formats."},{"location":"glossary/#hyperscan","title":"Hyperscan","text":"A high-performance multiple regex matching library by Intel: https://www.hyperscan.io/ We are using it for finding specific bit/byte pattern matching like magic headers."},{"location":"glossary/#unknown-chunk","title":"Unknown chunk","text":"A byte stream which none of our <code>Handler</code>s was able to recognize. They are carved to separate files, with the filename including their start and end offsets."},{"location":"glossary/#valid-chunk","title":"Valid chunk","text":"A <code>ValidChunk</code> is something that one of the <code>Handler</code>s found and we can extract."},{"location":"glossary/#recursion-depth","title":"Recursion depth","text":"unblob is processing input files recursively, which means if we extracted a file, that contains further files inside it, those will also be extracted, until the recursion depth is reached. Beyond that level, no further extraction will happen. For example, if a <code>tar.gz</code> contains a <code>zip</code> and a text file, the recursion depth will be 3: 1. gzip layer, 2. tar, 3. zip and text file."},{"location":"glossary/#multifile","title":"MultiFile","text":"<p>A set of files that were identified by a <code>DirectoryHandler</code> representing a format which consists of multiple files. <code>MultiFile</code> is extracted using a <code>DirectoryExtractor</code></p>"},{"location":"guide/","title":"User guide","text":""},{"location":"guide/#quickstart","title":"Quickstart","text":"<p>unblob has a very simple command line interface with sensible defaults. You just need to pass it a file you want to extract:</p> <pre><code>$ unblob alpine-minirootfs-3.16.1-x86_64.tar.gz\n2022-07-30 06:33.07 [info     ] Start processing file          file=openwrt-21.02.2-x86-64-generic-ext4-combined.img.gz pid=7092\n</code></pre> <p>It will make a new directory with the original filename appended with <code>_extract</code>:</p> <pre><code>$ ls -l\ntotal 2656\ndrwxrwxr-x 3 walkman walkman    4096 Jul 30 08:43 alpine-minirootfs-3.16.1-x86_64.tar.gz_extract\n-rw-r--r-- 1 walkman walkman 2711958 Jul 30 08:43 alpine-minirootfs-3.16.1-x86_64.tar.gz\n</code></pre> <p>And will extract all known file formats recursively until the specified recursion depth level (which is 10 by default):</p> <pre><code>$ tree -L 2\nalpine-minirootfs-3.16.1-x86_64.tar.gz_extract\n\u251c\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar\n\u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar_extract\n    \u251c\u2500\u2500 bin\n    \u251c\u2500\u2500 dev\n    \u251c\u2500\u2500 etc\n    \u251c\u2500\u2500 home\n    \u251c\u2500\u2500 lib\n    \u251c\u2500\u2500 media\n    \u251c\u2500\u2500 mnt\n    \u251c\u2500\u2500 opt\n    \u251c\u2500\u2500 proc\n    \u251c\u2500\u2500 root\n    \u251c\u2500\u2500 run\n    \u251c\u2500\u2500 sbin\n    \u251c\u2500\u2500 srv\n    \u251c\u2500\u2500 sys\n    \u251c\u2500\u2500 tmp\n    \u251c\u2500\u2500 usr\n    \u2514\u2500\u2500 var\n\n18 directories, 1 file\n</code></pre>"},{"location":"guide/#features","title":"Features","text":""},{"location":"guide/#metadata-extraction","title":"Metadata extraction","text":"<p>unblob can generate a metadata file about the extracted files in a JSON format by using the <code>--report</code> CLI option:</p> <pre><code>$ unblob --report alpine-report.json alpine-minirootfs-3.16.1-x86_64.tar.gz\n2022-07-30 07:06.59 [info     ] Start processing file          file=alpine-minirootfs-3.16.1-x86_64.tar.gz pid=13586\n2022-07-30 07:07.00 [info     ] JSON report written            path=alpine-report.json pid=13586\n\n$ cat alpine-report.json\n[\n  {\n    \"task\": {\n      \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz\",\n      \"depth\": 0,\n      \"chunk_id\": \"\"\n    },\n    \"reports\": [\n      {\n        \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz\",\n        \"size\": 2711958,\n        \"is_dir\": false,\n        \"is_file\": true,\n        \"is_link\": false,\n        \"link_target\": null,\n        \"__typename__\": \"StatReport\"\n      },\n      {\n        \"magic\": \"gzip compressed data, max compression, from Unix, original size modulo 2^32 5816320\\\\012- data\",\n        \"mime_type\": \"application/gzip\",\n        \"__typename__\": \"FileMagicReport\"\n      },\n      {\n        \"id\": \"13590:1\",\n        \"handler_name\": \"gzip\",\n        \"start_offset\": 0,\n        \"end_offset\": 2711958,\n        \"size\": 2711958,\n        \"is_encrypted\": false,\n        \"extraction_reports\": [],\n        \"__typename__\": \"ChunkReport\"\n      }\n    ],\n    \"subtasks\": [\n      {\n        \"path\": \"/home/walkman/Projects/unblob/demo/alpine-minirootfs-3.16.1-x86_64.tar.gz_extract\",\n        \"depth\": 1,\n        \"chunk_id\": \"13590:1\"\n      }\n    ]\n  },\n  ...\n]\n</code></pre>"},{"location":"guide/#randomness-calculation","title":"Randomness calculation","text":"<p>If you are analyzing an unknown file format, it might be useful to know the randomness of the contained files, so you can quickly see for example whether the file is encrypted or contains some random content.</p> <p>Let's make a file with fully random content at the start and end:</p> <pre><code>$ dd if=/dev/random of=random1.bin bs=10M count=1\n$ dd if=/dev/random of=random2.bin bs=10M count=1\n$ cat random1.bin alpine-minirootfs-3.16.1-x86_64.tar.gz random2.bin &gt; unknown-file\n</code></pre> <p>A nice ASCII randomness plot is drawn on verbose level 3:</p> <pre><code>$ unblob -vvv unknown-file | grep -C 15 \"Entropy distribution\"\n\n2024-10-30 10:52.03 [debug    ] Calculating chunk for pattern match handler=arc pid=1963719 real_offset=0x1685f5b start_offset=0x1685f5b\n2024-10-30 10:52.03 [debug    ] Header parsed                  header=&lt;arc_head archive_marker=0x1a, header_type=0x1, name=b'8\\xa7i&amp;po\\xc77\\xd5h\\x9a\\x9d\\xf1', size=0x26d171fa, date=0x1bfd, time=0xe03f, crc=-0x3b95, length=0x349997d5&gt; pid=1963719\n2024-10-30 10:52.03 [debug    ] Ended searching for chunks     all_chunks=[0xa00000-0xc96196] pid=1963719\n2024-10-30 10:52.03 [debug    ] Removed inner chunks           outer_chunk_count=1 pid=1963719 removed_inner_chunk_count=0\n2024-10-30 10:52.03 [warning  ] Found unknown Chunks           chunks=[0x0-0xa00000, 0xc96196-0x1696196] pid=1963719\n2024-10-30 10:52.03 [info     ] Extracting unknown chunk       chunk=0x0-0xa00000 path=unknown-file_extract/0-10485760.unknown pid=1963719\n2024-10-30 10:52.03 [debug    ] Carving chunk                  path=unknown-file_extract/0-10485760.unknown pid=1963719\n2024-10-30 10:52.03 [debug    ] Calculating randomness for file path=unknown-file_extract/0-10485760.unknown pid=1963719 size=0xa00000\n2024-10-30 10:52.03 [debug    ] Shannon entropy calculated     block_size=0x20000 highest=99.99 lowest=99.98 mean=99.98 path=unknown-file_extract/0-10485760.unknown pid=1963719 size=0xa00000\n2024-10-30 10:52.03 [debug    ] Chi square probability calculated block_size=0x20000 highest=97.88 lowest=3.17 mean=52.76 path=unknown-file_extract/0-10485760.unknown pid=1963719 size=0xa00000\n2024-10-30 10:52.03 [debug    ] Entropy chart                  chart=\n                              Randomness distribution\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n100\u2524 \u2022\u2022 Shannon entropy (%)        \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2670\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2502\n 90\u2524 \u2670\u2670 Chi square probability (%)   \u2670 \u2670 \u2670\u2670\u2670\u2670                    \u2670    \u2670  \u2670     \u2502\n 80\u2524\u2670 \u2670 \u2670\u2670  \u2670\u2670       \u2670\u2670       \u2670 \u2670   \u2670\u2670\u2670\u2670\u2670\u2670\u2670\u2670\u2670   \u2670           \u2670\u2670\u2670\u2670\u2670\u2670   \u2670\u2670 \u2670\u2670     \u2502\n 70\u2524\u2670\u2670\u2670\u2670  \u2670 \u2670 \u2670 \u2670   \u2670\u2670\u2670  \u2670 \u2670  \u2670 \u2670   \u2670\u2670\u2670\u2670\u2670\u2670\u2670\u2670\u2670  \u2670\u2670      \u2670 \u2670 \u2670   \u2670\u2670\u2670  \u2670\u2670\u2670\u2670\u2670\u2670     \u2502\n 60\u2524\u2670\u2670\u2670\u2670  \u2670\u2670  \u2670\u2670 \u2670 \u2670\u2670\u2670\u2670 \u2670 \u2670\u2670 \u2670  \u2670 \u2670 \u2670\u2670\u2670\u2670\u2670\u2670 \u2670\u2670 \u2670 \u2670     \u2670\u2670\u2670\u2670 \u2670   \u2670\u2670\u2670 \u2670\u2670\u2670\u2670\u2670\u2670\u2670     \u2502\n 50\u2524 \u2670\u2670\u2670  \u2670\u2670  \u2670\u2670 \u2670\u2670 \u2670\u2670\u2670\u2670  \u2670\u2670 \u2670  \u2670\u2670\u2670 \u2670\u2670\u2670\u2670\u2670\u2670  \u2670 \u2670 \u2670    \u2670\u2670\u2670\u2670\u2670 \u2670   \u2670\u2670\u2670 \u2670 \u2670\u2670\u2670\u2670\u2670  \u2670  \u2502\n 40\u2524  \u2670\u2670  \u2670\u2670   \u2670 \u2670\u2670 \u2670\u2670\u2670\u2670  \u2670\u2670 \u2670  \u2670\u2670\u2670 \u2670\u2670\u2670\u2670\u2670\u2670   \u2670\u2670  \u2670\u2670 \u2670\u2670\u2670\u2670\u2670\u2670 \u2670   \u2670\u2670\u2670 \u2670  \u2670\u2670\u2670\u2670 \u2670\u2670 \u2670\u2502\n 30\u2524   \u2670  \u2670\u2670     \u2670\u2670 \u2670\u2670\u2670\u2670  \u2670 \u2670\u2670  \u2670\u2670 \u2670\u2670 \u2670 \u2670\u2670    \u2670   \u2670 \u2670\u2670\u2670 \u2670 \u2670     \u2670\u2670 \u2670  \u2670\u2670\u2670 \u2670\u2670 \u2670 \u2502\n 20\u2524      \u2670\u2670     \u2670\u2670  \u2670\u2670\u2670  \u2670 \u2670\u2670   \u2670 \u2670\u2670    \u2670        \u2670 \u2670 \u2670 \u2670         \u2670    \u2670\u2670      \u2502\n 10\u2524       \u2670      \u2670    \u2670  \u2670  \u2670     \u2670\u2670    \u2670         \u2670                   \u2670\u2670      \u2502\n  0\u2524                                \u2670                                   \u2670      \u2502\n   \u2514\u2500\u252c\u2500\u2500\u252c\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2518\n   0 2  5 7 11   16  20 23 27 30  34  38 42   47  51   56 60 63   68 71  76 79\n                                   131072 bytes\n path=unknown-file_extract/0-10485760.unknown pid=1963719\n2024-10-30 10:52.03 [info     ] Extracting unknown chunk       chunk=0xc96196-0x1696196 path=unknown-file_extract/13197718-23683478.unknown pid=1963719\n2024-10-30 10:52.03 [debug    ] Carving chunk                  path=unknown-file_extract/13197718-23683478.unknown pid=1963719\n2024-10-30 10:52.03 [debug    ] Calculating randomness for file path=unknown-file_extract/13197718-23683478.unknown pid=1963719 size=0xa00000\n2024-10-30 10:52.03 [debug    ] Shannon entropy calculated     block_size=0x20000 highest=99.99 lowest=99.98 mean=99.98 path=unknown-file_extract/13197718-23683478.unknown pid=1963719 size=0xa00000\n2024-10-30 10:52.03 [debug    ] Chi square probability calculated block_size=0x20000 highest=99.03 lowest=0.23 mean=42.62 path=unknown-file_extract/13197718-23683478.unknown pid=1963719 size=0xa00000\n2024-10-30 10:52.03 [debug    ] Entropy chart                  chart=\n                              Randomness distribution\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n100\u2524 \u2022\u2022 Shannon entropy (%)        \u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2670\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2502\n 90\u2524 \u2670\u2670 Chi square probability (%)         \u2670           \u2670\u2670            \u2670         \u2502\n 80\u2524\u2670\u2670        \u2670\u2670    \u2670\u2670    \u2670               \u2670\u2670       \u2670   \u2670\u2670        \u2670  \u2670\u2670         \u2502\n 70\u2524\u2670 \u2670   \u2670  \u2670  \u2670  \u2670 \u2670    \u2670 \u2670    \u2670        \u2670\u2670      \u2670\u2670   \u2670\u2670\u2670   \u2670  \u2670\u2670  \u2670\u2670         \u2502\n 60\u2524  \u2670  \u2670\u2670 \u2670   \u2670 \u2670  \u2670  \u2670\u2670\u2670\u2670\u2670   \u2670\u2670        \u2670\u2670 \u2670\u2670   \u2670 \u2670  \u2670\u2670\u2670  \u2670\u2670 \u2670 \u2670  \u2670\u2670   \u2670     \u2502\n 50\u2524  \u2670 \u2670\u2670\u2670 \u2670   \u2670 \u2670  \u2670 \u2670 \u2670\u2670\u2670\u2670 \u2670 \u2670\u2670      \u2670 \u2670\u2670\u2670 \u2670   \u2670 \u2670  \u2670\u2670\u2670  \u2670\u2670 \u2670 \u2670  \u2670\u2670  \u2670\u2670   \u2670 \u2502\n 40\u2524  \u2670\u2670\u2670\u2670 \u2670\u2670    \u2670\u2670  \u2670 \u2670 \u2670\u2670  \u2670\u2670\u2670  \u2670\u2670\u2670  \u2670\u2670\u2670 \u2670\u2670 \u2670   \u2670  \u2670 \u2670\u2670 \u2670 \u2670\u2670 \u2670 \u2670 \u2670 \u2670 \u2670\u2670\u2670  \u2670\u2670 \u2502\n 30\u2524  \u2670\u2670\u2670\u2670 \u2670\u2670    \u2670\u2670   \u2670\u2670 \u2670\u2670   \u2670\u2670     \u2670\u2670\u2670\u2670\u2670 \u2670\u2670 \u2670   \u2670  \u2670 \u2670\u2670  \u2670\u2670\u2670 \u2670 \u2670 \u2670 \u2670 \u2670 \u2670  \u2670 \u2670\u2502\n 20\u2524   \u2670\u2670\u2670  \u2670     \u2670      \u2670\u2670   \u2670\u2670      \u2670\u2670\u2670\u2670 \u2670\u2670 \u2670   \u2670  \u2670 \u2670\u2670   \u2670\u2670 \u2670 \u2670\u2670  \u2670\u2670  \u2670  \u2670  \u2502\n 10\u2524     \u2670                \u2670    \u2670       \u2670 \u2670  \u2670 \u2670 \u2670\u2670   \u2670 \u2670\u2670     \u2670\u2670 \u2670\u2670   \u2670  \u2670 \u2670   \u2502\n  0\u2524                                           \u2670 \u2670    \u2670\u2670          \u2670       \u2670\u2670   \u2502\n   \u2514\u2500\u252c\u2500\u2500\u252c\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u252c\u2500\u2518\n   0 2  5 7 11   16  20 23 27 30  34  38 42   47  51   56 60 63   68 71  76 79\n                                   131072 bytes\n</code></pre>"},{"location":"guide/#skip-extraction-with-file-magic","title":"Skip extraction with file magic","text":"<p>The extraction process can be faster and produce fewer false positives if we just ignore some files, which we know will not contain meaningful results, or it makes no sense to extract them. Examples of such file formats are SQLite, images, fonts, or PDF documents.</p> <p>We have a default for the skip list, but you can change it with the <code>--skip-magic</code> CLI option. Here is a silly example:</p> <pre><code>$ unblob --skip-magic \"POSIX tar archive\" alpine-minirootfs-3.16.1-x86_64.tar.gz\n2022-07-30 07:18.09 [info ] Start processing file file=alpine-minirootfs-3.16.1-x86_64.tar.gz pid=14971\n\n$ tree .\n\u251c\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar.gz\n\u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar.gz_extract\n\u2514\u2500\u2500 alpine-minirootfs-3.16.1-x86_64.tar\n</code></pre> <p>Here gzip has been extracted, but we skipped the tar extraction, so no other files have been extracted further.</p>"},{"location":"guide/#full-command-line-interface","title":"Full Command line interface","text":"<pre><code>Usage: unblob [OPTIONS] FILE\n\n  A tool for getting information out of any kind of binary blob.\n\n  You also need these extractor commands to be able to extract the supported\n  file types: 7z, debugfs, jefferson, lz4, lziprecover, lzop, sasquatch,\n  sasquatch-v4be, simg2img, ubireader_extract_files, ubireader_extract_images,\n  unar, zstd\n\n  NOTE: Some older extractors might not be compatible.\n\nOptions:\n  -e, --extract-dir DIRECTORY     Extract the files to this directory. Will be\n                                  created if doesn't exist.\n  -f, --force                     Force extraction even if outputs already\n                                  exist (they are removed).\n  -d, --depth INTEGER RANGE       Recursion depth. How deep should we extract\n                                  containers.  [default: 10; x&gt;=1]\n  -n, --entropy-depth INTEGER RANGE\n                                  Entropy calculation depth. How deep should\n                                  we calculate entropy for unknown files? 1\n                                  means input files only, 0 turns it off.\n                                  [default: 1; x&gt;=0]\n  -P, --plugins-path PATH         Load plugins from the provided path.\n  -S, --skip-magic TEXT           Skip processing files with given magic\n                                  prefix  [default: BFLT, JPEG, GIF, PNG,\n                                  SQLite, compiled Java class, TrueType Font\n                                  data, PDF document, magic binary file, MS\n                                  Windows icon resource, PE32+ executable (EFI\n                                  application)]\n  -p, --process-num INTEGER RANGE\n                                  Number of worker processes to process files\n                                  parallelly.  [default: 12; x&gt;=1]\n  --report PATH                   File to store metadata generated during the\n                                  extraction process (in JSON format).\n  -k, --keep-extracted-chunks     Keep extracted chunks\n  -v, --verbose                   Verbosity level, counting, maximum level: 3\n                                  (use: -v, -vv, -vvv)\n  --show-external-dependencies    Shows commands needs to be available for\n                                  unblob to work properly\n  -h, --help                      Show this message and exit.\n</code></pre>"},{"location":"handlers/","title":"Handlers","text":"All supported formats Format Type Fully supported? <code>7-ZIP</code> ARCHIVE <code>ANDROID EROFS</code> FILESYSTEM <code>ANDROID SPARSE</code> FILESYSTEM <code>AR</code> ARCHIVE <code>ARC</code> ARCHIVE <code>ARJ</code> ARCHIVE <code>AUTEL ECC</code> ARCHIVE <code>BZIP2</code> COMPRESSION <code>CAB</code> ARCHIVE <code>COMPRESS</code> COMPRESSION <code>CPIO (BINARY)</code> ARCHIVE <code>CPIO (PORTABLE ASCII CRC)</code> ARCHIVE <code>CPIO (PORTABLE ASCII)</code> ARCHIVE <code>CPIO (PORTABLE OLD ASCII)</code> ARCHIVE <code>CRAMFS</code> FILESYSTEM <code>D-LINK ENCRPTED_IMG</code> ARCHIVE <code>D-LINK SHRS</code> ARCHIVE <code>DMG</code> ARCHIVE <code>ELF (32-BIT)</code> EXECUTABLE <code>ELF (64-BIT)</code> EXECUTABLE <code>ENGENIUS</code> ARCHIVE <code>EXTFS</code> FILESYSTEM <code>FAT</code> FILESYSTEM <code>GZIP</code> COMPRESSION <code>GZIP (MULTI-VOLUME)</code> COMPRESSION <code>HP BDL</code> ARCHIVE <code>HP IPKG</code> ARCHIVE <code>INSTAR BNEG</code> ARCHIVE <code>INSTAR HD</code> ARCHIVE <code>ISO 9660</code> FILESYSTEM <code>JFFS2 (NEW)</code> FILESYSTEM <code>JFFS2 (OLD)</code> FILESYSTEM <code>LZ4</code> COMPRESSION <code>LZ4 (LEGACY)</code> COMPRESSION <code>LZ4 (SKIPPABLE)</code> COMPRESSION <code>LZH</code> COMPRESSION <code>LZIP</code> COMPRESSION <code>LZMA</code> COMPRESSION <code>LZO</code> COMPRESSION <code>MULTI-SEVENZIP</code> ARCHIVE <code>NETGEAR CHK</code> ARCHIVE <code>NETGEAR TRX V1</code> ARCHIVE <code>NETGEAR TRX V2</code> ARCHIVE <code>NTFS</code> FILESYSTEM <code>PARTCLONE</code> ARCHIVE <code>QNAP NAS</code> ARCHIVE <code>RAR</code> ARCHIVE <code>ROMFS</code> FILESYSTEM <code>SQUASHFS (V1)</code> FILESYSTEM <code>SQUASHFS (V2)</code> FILESYSTEM <code>SQUASHFS (V3)</code> FILESYSTEM <code>SQUASHFS (V3-BROADCOM)</code> FILESYSTEM <code>SQUASHFS (V3-DDWRT)</code> FILESYSTEM <code>SQUASHFS (V3-NON-STANDARD)</code> FILESYSTEM <code>SQUASHFS (V4-BE)</code> FILESYSTEM <code>SQUASHFS (V4-LE)</code> FILESYSTEM <code>STUFFIT SIT</code> ARCHIVE <code>STUFFIT SIT (V5)</code> ARCHIVE <code>TAR (UNIX)</code> ARCHIVE <code>TAR (USTAR)</code> ARCHIVE <code>UBI</code> FILESYSTEM <code>UBIFS</code> FILESYSTEM <code>UZIP</code> COMPRESSION <code>XIAOMI HDR1</code> ARCHIVE <code>XIAOMI HDR2</code> ARCHIVE <code>XZ</code> COMPRESSION <code>YAFFS</code> FILESYSTEM <code>ZIP</code> ARCHIVE <code>ZLIB</code> COMPRESSION <code>ZSTD</code> COMPRESSION"},{"location":"handlers/#7-zip","title":"7-Zip","text":"<p>Fully supported</p> DescriptionReferences <p>The 7-Zip file format is a compressed archive format with high compression ratios, supporting multiple algorithms, CRC checks, and multi-volume archives.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>7-Zip Technical Documentation</li> </ul>"},{"location":"handlers/#android-erofs","title":"Android EROFS","text":"<p>Fully supported</p> DescriptionReferences <p>EROFS (Enhanced Read-Only File System) is a lightweight, high-performance file system designed for read-only use cases, commonly used in Android and Linux. It features compression support, metadata efficiency, and a fixed superblock structure.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: Google</li> </ul> <ul> <li>EROFS Documentation</li> <li>EROFS Wikipedia</li> </ul>"},{"location":"handlers/#android-sparse","title":"Android Sparse","text":"<p>Fully supported</p> DescriptionReferences <p>Android sparse images are a file format used to efficiently store disk images by representing sequences of zero blocks compactly. The format includes a file header, followed by chunk headers and data, with support for raw, fill, and 'don't care' chunks.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: Google</li> </ul> <ul> <li>Android Sparse Image Format Documentation</li> <li>simg2img Tool</li> </ul>"},{"location":"handlers/#ar","title":"AR","text":"<p>Fully supported</p> DescriptionReferences <p>Unix AR (archive) files are used to store multiple files in a single archive with a simple header format.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>Unix AR File Format Documentation</li> </ul>"},{"location":"handlers/#arc","title":"ARC","text":"<p>Fully supported</p> DescriptionReferences <p>ARC is a legacy archive format used to store multiple files with metadata such as file size, creation date, and CRC.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>ARC File Format Documentation</li> </ul>"},{"location":"handlers/#arj","title":"ARJ","text":"<p>Fully supported</p> DescriptionReferences <p>ARJ is a legacy compressed archive formats used to store multiple files with metadata such as file size, creation date, and CRC.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>ARJ File Format Documentation</li> <li>ARJ Technical Information</li> </ul>"},{"location":"handlers/#autel-ecc","title":"Autel ECC","text":"<p>Fully supported</p> DescriptionReferences <p>Autel ECC files consist of a custom header followed by encrypted data blocks. The header includes metadata such as magic bytes, file size, and copyright information.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Autel</li> </ul> <ul> <li>Autel ECC Decryption Script (Sector7)</li> </ul>"},{"location":"handlers/#bzip2","title":"bzip2","text":"<p>Fully supported</p> DescriptionReferences <p>The bzip2 format is a block-based compression format that uses the Burrows-Wheeler transform and Huffman coding for high compression efficiency. Each stream starts with a header and consists of one or more compressed blocks, ending with a footer containing a checksum.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>bzip2 File Format Documentation</li> <li>bzip2 Technical Specification</li> </ul>"},{"location":"handlers/#cab","title":"CAB","text":"<p>Fully supported</p> DescriptionReferences <p>Microsoft Cabinet (CAB) archive files are used for compressed file storage and software installation.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Microsoft</li> </ul> <ul> <li>Microsoft Cabinet File Format Documentation</li> <li>Ubuntu Manual - cabextract</li> </ul>"},{"location":"handlers/#compress","title":"compress","text":"<p>Fully supported</p> DescriptionReferences <p>Unix compress files use the Lempel-Ziv-Welch (LZW) algorithm for data compression and are identified by a 2-byte magic number (0x1F 0x9D). This format supports optional block compression and variable bit lengths ranging from 9 to 16 bits.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>Unix Compress File Format Documentation</li> <li>LZW Compression Algorithm</li> </ul>"},{"location":"handlers/#cpio-binary","title":"CPIO (binary)","text":"<p>Fully supported</p> DescriptionReferences <p>CPIO (Copy In, Copy Out) is an archive file format used for bundling files and directories along with their metadata. It is commonly used in Unix-like systems for creating backups or transferring files, and supports various encoding formats including binary and ASCII.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>GNU CPIO Manual</li> </ul>"},{"location":"handlers/#cpio-portable-ascii-crc","title":"CPIO (portable ASCII CRC)","text":"<p>Fully supported</p> DescriptionReferences <p>CPIO (Copy In, Copy Out) is an archive file format used for bundling files and directories along with their metadata. It is commonly used in Unix-like systems for creating backups or transferring files, and supports various encoding formats including binary and ASCII.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>GNU CPIO Manual</li> </ul>"},{"location":"handlers/#cpio-portable-ascii","title":"CPIO (portable ASCII)","text":"<p>Fully supported</p> DescriptionReferences <p>CPIO (Copy In, Copy Out) is an archive file format used for bundling files and directories along with their metadata. It is commonly used in Unix-like systems for creating backups or transferring files, and supports various encoding formats including binary and ASCII.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>GNU CPIO Manual</li> </ul>"},{"location":"handlers/#cpio-portable-old-ascii","title":"CPIO (portable old ASCII)","text":"<p>Fully supported</p> DescriptionReferences <p>CPIO (Copy In, Copy Out) is an archive file format used for bundling files and directories along with their metadata. It is commonly used in Unix-like systems for creating backups or transferring files, and supports various encoding formats including binary and ASCII.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>GNU CPIO Manual</li> </ul>"},{"location":"handlers/#cramfs","title":"CramFS","text":"<p>Fully supported</p> DescriptionReferences <p>CramFS is a lightweight, read-only file system format designed for simplicity and efficiency in embedded systems. It uses zlib compression for file data and stores metadata in a compact, contiguous structure.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>CramFS Documentation</li> <li>CramFS Wikipedia</li> </ul>"},{"location":"handlers/#d-link-encrpted_img","title":"D-Link encrpted_img","text":"<p>Fully supported</p> DescriptionReferences <p>A binary format used by D-Link to store encrypted firmware or data. It consists of a custom 12-byte magic header followed by the encrypted payload.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: D-Link</li> </ul> <ul> <li>How-To: Extracting Decryption Keys for D-Link</li> </ul>"},{"location":"handlers/#d-link-shrs","title":"D-Link SHRS","text":"<p>Fully supported</p> DescriptionReferences <p>SHRS is a D-Link firmware format with a custom header containing metadata, SHA-512 digests, and AES-CBC encryption parameters. The firmware data is encrypted using a fixed key and IV stored in the header.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: D-Link</li> </ul> <ul> <li>Breaking the D-Link DIR3060 Firmware Encryption - Recon - Part 1</li> </ul>"},{"location":"handlers/#dmg","title":"DMG","text":"<p>Fully supported</p> DescriptionReferences <p>Apple Disk Image (DMG) files are commonly used on macOS for software distribution and disk image storage.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Apple</li> </ul> <ul> <li>Apple Disk Image Format Documentation</li> </ul>"},{"location":"handlers/#elf-32-bit","title":"ELF (32-bit)","text":"<p>Fully supported</p> DescriptionReferences <p>The 32-bit ELF (Executable and Linkable Format) is a binary file format used for executables, object code, shared libraries, and core dumps. It supports 32-bit addressing and includes headers for program and section information.</p> <ul> <li>Handler type: Executable</li> </ul> <ul> <li>ELF File Format Specification</li> <li>ELF Wikipedia</li> </ul>"},{"location":"handlers/#elf-64-bit","title":"ELF (64-bit)","text":"<p>Fully supported</p> DescriptionReferences <p>The 64-bit ELF (Executable and Linkable Format) is a binary file format used for executables, object code, shared libraries, and core dumps. It supports 64-bit addressing and includes headers for program and section information.</p> <ul> <li>Handler type: Executable</li> </ul> <ul> <li>ELF File Format Specification</li> <li>ELF Wikipedia</li> </ul>"},{"location":"handlers/#engenius","title":"Engenius","text":"<p>Partially supported</p> DescriptionReferencesLimitations <p>Engenius firmware files contain a custom header with metadata, followed by encrypted data using an XOR cipher.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Engenius</li> </ul> <ul> <li>enfringement - Tools for working with EnGenius WiFi hardware.</li> </ul> <ul> <li>Does not support all firmware versions.</li> </ul>"},{"location":"handlers/#extfs","title":"ExtFS","text":"<p>Fully supported</p> DescriptionReferences <p>ExtFS (Ext2/Ext3/Ext4) is a family of journaling file systems commonly used in Linux-based operating systems. It supports features like large file sizes, extended attributes, and journaling for improved reliability.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>Ext4 Documentation</li> <li>ExtFS Wikipedia</li> </ul>"},{"location":"handlers/#fat","title":"FAT","text":"<p>Fully supported</p> DescriptionReferences <p>FAT (File Allocation Table) is a file system format used for organizing and managing files on storage devices, supporting FAT12, FAT16, and FAT32 variants. It uses a table to map file clusters, enabling efficient file storage and retrieval.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>FAT Wikipedia</li> </ul>"},{"location":"handlers/#gzip","title":"GZIP","text":"<p>Fully supported</p> DescriptionReferences <p>GZIP is a compressed file format that uses the DEFLATE algorithm and includes metadata such as original file name and modification time. It is commonly used for efficient file storage and transfer.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>GZIP File Format Specification</li> <li>GZIP Wikipedia</li> </ul>"},{"location":"handlers/#gzip-multi-volume","title":"GZIP (multi-volume)","text":"<p>Fully supported</p> DescriptionReferences <p>GZIP is a compressed file format that uses the DEFLATE algorithm and includes metadata such as original file name and modification time. It is commonly used for efficient file storage and transfer.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>GZIP File Format Specification</li> <li>GZIP Wikipedia</li> </ul>"},{"location":"handlers/#hp-bdl","title":"HP BDL","text":"<p>Fully supported</p> DescriptionReferences <p>The HP BDL format is a firmware archive containing a custom header and a table of contents that specifies offsets and sizes of embedded firmware components. It includes metadata such as release, brand, device ID, version, and revision.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: HP</li> </ul> <ul> <li>hpbdl</li> </ul>"},{"location":"handlers/#hp-ipkg","title":"HP IPKG","text":"<p>Fully supported</p> DescriptionReferences <p>HP IPKG firmware archives consist of a custom header, followed by a table of contents and file entries. Each entry specifies metadata such as file name, offset, size, and CRC32 checksum.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: HP</li> </ul> <ul> <li>hpbdl</li> </ul>"},{"location":"handlers/#instar-bneg","title":"Instar BNEG","text":"<p>Fully supported</p> DescriptionReferences <p>BNEG firmware files consist of a custom header followed by two partitions containing firmware components. The header specifies metadata such as magic value, version, and partition sizes.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Instar</li> </ul>"},{"location":"handlers/#instar-hd","title":"Instar HD","text":"<p>Fully supported</p> DescriptionReferences <p>Instar HD firmware files are modified ZIP archives with non-standard local file headers, central directory headers, and end-of-central-directory records. These modifications include custom magic bytes to differentiate them from standard ZIP files.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Instar</li> </ul>"},{"location":"handlers/#iso-9660","title":"ISO 9660","text":"<p>Fully supported</p> DescriptionReferences <p>ISO 9660 is a file system standard for optical disc media, defining a volume descriptor structure and directory hierarchy. It is widely used for CD-ROMs and supports cross-platform compatibility.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>ISO 9660 Specification</li> <li>ISO 9660 Wikipedia</li> </ul>"},{"location":"handlers/#jffs2-new","title":"JFFS2 (new)","text":"<p>Fully supported</p> DescriptionReferences <p>JFFS2 (Journaling Flash File System version 2) is a log-structured file system for flash memory devices, using an older magic number to identify its nodes. It organizes data into nodes with headers containing metadata and CRC checks for integrity.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>JFFS2 Documentation</li> <li>JFFS2 Wikipedia</li> </ul>"},{"location":"handlers/#jffs2-old","title":"JFFS2 (old)","text":"<p>Fully supported</p> DescriptionReferences <p>JFFS2 (Journaling Flash File System version 2) is a log-structured file system for flash memory devices, using an older magic number to identify its nodes. It organizes data into nodes with headers containing metadata and CRC checks for integrity.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>JFFS2 Documentation</li> <li>JFFS2 Wikipedia</li> </ul>"},{"location":"handlers/#lz4","title":"LZ4","text":"<p>Fully supported</p> DescriptionReferences <p>LZ4 is a high-speed lossless compression algorithm designed for real-time data compression with minimal memory usage.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZ4 Frame Format Documentation</li> <li>LZ4 Wikipedia</li> </ul>"},{"location":"handlers/#lz4-legacy","title":"LZ4 (legacy)","text":"<p>Fully supported</p> DescriptionReferences <p>LZ4 legacy format is an older framing format used prior to the LZ4 Frame specification, featuring a simpler structure and no support for skippable frames or extensive metadata. Unlike the default LZ4 Frame format, it lacks built-in checksums, versioning, or block independence flags, making it less robust and primarily used for backward compatibility.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZ4 Frame Format Documentation</li> <li>LZ4 Wikipedia</li> </ul>"},{"location":"handlers/#lz4-skippable","title":"LZ4 (skippable)","text":"<p>Fully supported</p> DescriptionReferences <p>LZ4 skippable format is designed to encapsulate arbitrary data within an LZ4 stream allowing compliant parsers to skip over it safely. This format does not contain compressed data itself but is often used for embedding metadata or non-LZ4 content alongside standard frames.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZ4 Frame Format Documentation</li> <li>LZ4 Wikipedia</li> </ul>"},{"location":"handlers/#lzh","title":"LZH","text":"<p>Fully supported</p> DescriptionReferences <p>LZH is a legacy archive format that uses various compression methods such as '-lh0-' and '-lh5-'. It was widely used in Japan and on older systems for compressing and archiving files.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZH Compression Format</li> </ul>"},{"location":"handlers/#lzip","title":"Lzip","text":"<p>Fully supported</p> DescriptionReferences <p>Lzip is a lossless compressed file format based on the LZMA algorithm. It features a simple header, CRC-checked integrity, and efficient compression for large files.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>Lzip File Format Documentation</li> <li>Lzip Wikipedia</li> </ul>"},{"location":"handlers/#lzma","title":"LZMA","text":"<p>Fully supported</p> DescriptionReferences <p>LZMA is a compression format based on the Lempel-Ziv-Markov chain algorithm, offering high compression ratios and efficient decompression. It is commonly used in standalone <code>.lzma</code> files and embedded in other formats like 7z.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZMA File Format Documentation</li> <li>LZMA Wikipedia</li> </ul>"},{"location":"handlers/#lzo","title":"LZO","text":"<p>Fully supported</p> DescriptionReferences <p>LZO is a data compression format featuring a simple header structure and optional checksum verification. It is optimized for fast decompression and supports various compression levels and flags for additional metadata.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>LZO File Format Documentation</li> <li>LZO Wikipedia</li> </ul>"},{"location":"handlers/#multi-sevenzip","title":"multi-sevenzip","text":"<p>Fully supported</p> DescriptionReferences <p>The 7-Zip file format is a compressed archive format with high compression ratios, supporting multiple algorithms, CRC checks, and multi-volume archives.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>7-Zip Technical Documentation</li> </ul>"},{"location":"handlers/#netgear-chk","title":"Netgear CHK","text":"<p>Fully supported</p> DescriptionReferences <p>Netgear CHK firmware files consist of a custom header containing metadata and checksums, followed by kernel and root filesystem partitions. The header includes fields for partition sizes, checksums, and a board identifier.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Netgear</li> </ul> <ul> <li>CHK Image Format Image Builder Tool for the R7800 Series</li> </ul>"},{"location":"handlers/#netgear-trx-v1","title":"Netgear TRX v1","text":"<p>Fully supported</p> DescriptionReferences <p>Netgear TRX v1 firmware format includes a custom header with partition offsets and a CRC32 checksum for integrity verification. It supports up to three partitions defined in the header.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Netgear</li> </ul>"},{"location":"handlers/#netgear-trx-v2","title":"Netgear TRX v2","text":"<p>Fully supported</p> DescriptionReferences <p>Netgear TRX v2 firmware format includes a custom header with partition offsets and a CRC32 checksum for integrity verification. It supports up to four partitions defined in the header.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Netgear</li> </ul>"},{"location":"handlers/#ntfs","title":"NTFS","text":"<p>Fully supported</p> DescriptionReferences <p>NTFS (New Technology File System) is a proprietary file system developed by Microsoft, featuring metadata support, advanced data structures, and journaling for reliability. It is commonly used in Windows operating systems for efficient storage and retrieval of files.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: Microsoft</li> </ul> <ul> <li>NTFS Wikipedia</li> </ul>"},{"location":"handlers/#partclone","title":"Partclone","text":"<p>Fully supported</p> DescriptionReferences <p>Partclone is a utility used for backing up and restoring partitions. Many cloning tools (such as Clonezilla) rely on it to create block-level images that include filesystem metadata.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>Partclone GitHub Repository</li> <li>Clonezilla Official Documentation</li> </ul>"},{"location":"handlers/#qnap-nas","title":"QNAP NAS","text":"<p>Fully supported</p> DescriptionReferences <p>QNAP NAS firmware files consist of a custom header, encrypted data sections, and a footer marking the end of the encrypted stream. The header contains metadata such as device ID, firmware version, and encryption details.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: QNAP</li> </ul>"},{"location":"handlers/#rar","title":"RAR","text":"<p>Partially supported</p> DescriptionReferencesLimitations <p>RAR archive files are commonly used for compressed data storage. They can contain multiple files and directories, and support various compression methods.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>RAR 4.x File Format Documentation</li> <li>RAR 5.x File Format Documentation</li> </ul> <ul> <li>Does not support encrypted RAR files.</li> </ul>"},{"location":"handlers/#romfs","title":"RomFS","text":"<p>Fully supported</p> DescriptionReferences <p>RomFS is a simple, space-efficient, read-only file system format designed for embedded systems. It features 16-byte alignment, minimal metadata overhead, and supports basic file types like directories, files, symlinks, and devices.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>RomFS Documentation</li> <li>RomFS Wikipedia</li> </ul>"},{"location":"handlers/#squashfs-v1","title":"SquashFS (v1)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 1 is a compressed, read-only file system format designed for minimal storage usage. It is commonly used in embedded systems and early Linux distributions.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"handlers/#squashfs-v2","title":"SquashFS (v2)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 2 is a compressed, read-only file system format designed for minimal storage usage. It builds upon version 1 with additional features and improvements for embedded systems and Linux distributions.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"handlers/#squashfs-v3","title":"SquashFS (v3)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 3 is a compressed, read-only file system format designed for minimal storage usage. It is widely used in embedded systems and Linux distributions for efficient storage and fast access.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"handlers/#squashfs-v3-broadcom","title":"SquashFS (v3-Broadcom)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 3 Broadcom is a variant of the SquashFS v3 format used in Broadcom firmware. It features a unique magic number and may include specific optimizations for Broadcom devices.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: Broadcom</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"handlers/#squashfs-v3-ddwrt","title":"SquashFS (v3-DDWRT)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 3 DD-WRT is a variant of the SquashFS v3 format used in DD-WRT firmware. It features a unique magic number and may include specific optimizations for embedded systems.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: DDWRT</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"handlers/#squashfs-v3-non-standard","title":"SquashFS (v3-non-standard)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 3 is a compressed, read-only file system format designed for minimal storage usage. It is widely used in embedded systems and Linux distributions for efficient storage and fast access.</p> <ul> <li>Handler type: FileSystem</li> <li>Vendor: unknown</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"handlers/#squashfs-v4-be","title":"SquashFS (v4-BE)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 4 is a compressed, read-only file system format designed for minimal storage usage and fast access. It supports both big-endian and little-endian formats and is widely used in embedded systems and Linux distributions.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"handlers/#squashfs-v4-le","title":"SquashFS (v4-LE)","text":"<p>Fully supported</p> DescriptionReferences <p>SquashFS version 4 is a compressed, read-only file system format designed for minimal storage usage and fast access. It is widely used in embedded systems and Linux distributions for efficient storage management.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>SquashFS Documentation</li> <li>SquashFS Wikipedia</li> </ul>"},{"location":"handlers/#stuffit-sit","title":"Stuffit SIT","text":"<p>Fully supported</p> DescriptionReferences <p>StuffIt SIT archives is a legacy compressed archive format commonly used on macOS and earlier Apple systems.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: StuffIt Technologies</li> </ul> <ul> <li>StuffIt SIT File Format Documentation</li> </ul>"},{"location":"handlers/#stuffit-sit-v5","title":"Stuffit SIT (v5)","text":"<p>Fully supported</p> DescriptionReferences <p>StuffIt SIT archives is a legacy compressed archive format commonly used on macOS and earlier Apple systems.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: StuffIt Technologies</li> </ul> <ul> <li>StuffIt SIT File Format Documentation</li> </ul>"},{"location":"handlers/#tar-unix","title":"TAR (Unix)","text":"<p>Fully supported</p> DescriptionReferences <p>Unix tar files are a widely used archive format for storing files and directories with metadata.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>Unix Tar Format Documentation</li> <li>GNU Tar Manual</li> </ul>"},{"location":"handlers/#tar-ustar","title":"TAR (USTAR)","text":"<p>Fully supported</p> DescriptionReferences <p>USTAR (Uniform Standard Tape Archive) tar files are extensions of the original tar format with additional metadata fields.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>USTAR Format Documentation</li> <li>POSIX Tar Format Specification</li> </ul>"},{"location":"handlers/#ubi","title":"UBI","text":"<p>Fully supported</p> DescriptionReferences <p>UBI (Unsorted Block Image) is a volume management system for raw flash devices, providing wear leveling and bad block management. It operates as a layer between the MTD subsystem and higher-level filesystems like UBIFS.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>UBI Documentation</li> <li>UBI Wikipedia</li> </ul>"},{"location":"handlers/#ubifs","title":"UBIFS","text":"<p>Fully supported</p> DescriptionReferences <p>UBIFS (Unsorted Block Image File System) is a flash file system designed for raw flash memory, providing wear leveling, error correction, and power failure resilience. It operates on top of UBI volumes, which manage flash blocks on raw NAND or NOR flash devices.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>UBIFS Documentation</li> <li>UBIFS Wikipedia</li> </ul>"},{"location":"handlers/#uzip","title":"UZIP","text":"<p>Fully supported</p> DescriptionReferences <p>FreeBSD UZIP is a block-based compressed disk image format. It uses a table of contents to index compressed blocks, supporting ZLIB, LZMA, and ZSTD compression algorithms.</p> <ul> <li>Handler type: Compression</li> <li>Vendor: FreeBSD</li> </ul> <ul> <li>FreeBSD UZIP Documentation</li> </ul>"},{"location":"handlers/#xiaomi-hdr1","title":"Xiaomi HDR1","text":"<p>Fully supported</p> DescriptionReferences <p>Xiaomi HDR1 firmware files feature a custom header containing metadata, CRC32 checksum, and blob offsets for embedded data. These files are used in Xiaomi devices for firmware updates.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Xiaomi</li> </ul>"},{"location":"handlers/#xiaomi-hdr2","title":"Xiaomi HDR2","text":"<p>Fully supported</p> DescriptionReferences <p>Xiaomi HDR2 firmware files feature a custom header with metadata, CRC32 checksum, and blob offsets for embedded data. These files also include additional fields for device ID and region information.</p> <ul> <li>Handler type: Archive</li> <li>Vendor: Xiaomi</li> </ul>"},{"location":"handlers/#xz","title":"XZ","text":"<p>Fully supported</p> DescriptionReferences <p>XZ is a compressed file format that uses the LZMA2 algorithm for high compression efficiency. It is designed for general-purpose data compression with support for integrity checks and padding for alignment.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>XZ File Format Specification</li> <li>XZ Wikipedia</li> </ul>"},{"location":"handlers/#yaffs","title":"YAFFS","text":"<p>Fully supported</p> DescriptionReferences <p>YAFFS (Yet Another Flash File System) is a log-structured file system designed for NAND flash memory, storing data in fixed-size chunks with associated metadata. It supports features like wear leveling, error correction, and efficient handling of power loss scenarios.</p> <ul> <li>Handler type: FileSystem</li> </ul> <ul> <li>YAFFS Documentation</li> <li>YAFFS Wikipedia</li> </ul>"},{"location":"handlers/#zip","title":"ZIP","text":"<p>Partially supported</p> DescriptionReferencesLimitations <p>ZIP is a widely used archive file format that supports multiple compression methods, file spanning, and optional encryption. It includes metadata such as file names, sizes, and timestamps, and supports both standard and ZIP64 extensions for large files.</p> <ul> <li>Handler type: Archive</li> </ul> <ul> <li>ZIP File Format Specification</li> <li>ZIP64 Format Specification</li> </ul> <ul> <li>Does not support encrypted ZIP files.</li> </ul>"},{"location":"handlers/#zlib","title":"zlib","text":"<p>Fully supported</p> DescriptionReferences <p>The zlib format is a compressed data format based on the DEFLATE algorithm, often used for data compression in various applications. It includes a lightweight header and checksum for data integrity.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>zlib File Format Specification</li> <li>zlib Wikipedia</li> </ul>"},{"location":"handlers/#zstd","title":"ZSTD","text":"<p>Fully supported</p> DescriptionReferences <p>Zstandard (ZSTD) is a fast lossless compression algorithm with high compression ratios, designed for modern data storage and transfer. Its file format includes a frame structure with optional dictionary support and checksums for data integrity.</p> <ul> <li>Handler type: Compression</li> </ul> <ul> <li>Zstandard File Format Specification</li> <li>Zstandard Wikipedia</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Unblob consists of two main parts:</p> <ul> <li>unblob, the Python package.</li> <li>extractor command line tools like <code>7zip</code>, <code>unar</code>, etc. (See Extractors for explanation.)</li> </ul> <p>All of these need to be installed to make unblob fully functional. Depending the packaging solution you choose, you might need to install external extractors manually.</p>"},{"location":"installation/#python-package","title":"Python package","text":"<p>unblob can be installed (without the extractors) from PyPI (Python Package Index). This might be the easiest method, depending on whether you have Python 3 installed already.</p> <ol> <li> <p>First, install the Python package:</p> <pre><code>python3 -m pip install --user unblob\n</code></pre> <p>This will install the <code>unblob</code> script in <code>~/.local/bin</code>. You can put that directory in your <code>PATH</code> environment variable, or call it directly.</p> <p>Warning</p> <p>System-wide installation (with <code>sudo</code>) is not recommended, because it can potentially break your system.</p> </li> <li> <p>Make sure to install extractors.</p> </li> <li> <p>Check that everything works correctly:</p> <pre><code>unblob --show-external-dependencies\n</code></pre> </li> </ol>"},{"location":"installation/#kali-linux","title":"Kali Linux","text":"<p>If you're on Kali Linux, unblob is available through the distribution repository. You can install it with:</p> <pre><code>apt install unblob\n</code></pre>"},{"location":"installation/#docker-image","title":"Docker image","text":"<p>unblob can be used right away from a <code>docker</code> image: <code>ghcr.io/onekey-sec/unblob:latest</code>, which contains everything needed to run unblob, even the extractors.</p> <p>The <code>--pull always</code> option is recommended, because the project is currently under heavy development, so we expect frequent changes.</p> <p>The extracted files will be in the <code>/data/output</code> folder inside the container. Mount your host directory where you want to see the extracted files there:</p> <pre><code>docker run \\\n  --rm \\\n  --pull always \\\n  -v /path/to/extract-dir/on/host:/data/output \\\n  -v /path/to/files/on/host:/data/input \\\nghcr.io/onekey-sec/unblob:latest /data/input/path/to/file\n</code></pre> <p>Help on usage:</p> <pre><code>docker run --rm --pull always ghcr.io/onekey-sec/unblob:latest --help\n</code></pre> <p>\u26a0\ufe0f  When you bind mount directories from the host in the container using the <code>-v</code> option, you must make sure that they are owned by the same <code>uid:gid</code> pair otherwise you'll get into permission issues.</p> <p>If you're on a multi-user Linux host (i.e. your $UID is &gt; 1000), it's recommended you launch the container this way to map the container user's UID to yours:</p> <pre><code>docker run \\\n  --rm \\\n  --pull always \\\n  -u $UID:$GID\n  -v /path/to/extract-dir/on/host:/data/output \\\n  -v /path/to/files/on/host:/data/input \\\nghcr.io/onekey-sec/unblob:latest /data/input/path/to/file\n</code></pre>"},{"location":"installation/#nix-package","title":"Nix package","text":"<p>You can install stable releases of unblob directly from nixpkgs using your preferred package installation method, e.g:</p> <p>For a system-wide installation on NixOS:</p> <pre><code># configuration.nix\n\n{ pkgs, ...}:\n\n{\n  environment.systemPackages = [ pkgs.unblob ];\n}\n</code></pre> <p>For home-manager:</p> <pre><code># home.nix\n\n{ pkgs, ...}:\n\n{\n  home.packages = [ pkgs.unblob ];\n}\n</code></pre> <p>Using <code>nix-env</code>:</p> <pre><code>$ nix-env -iA nixpkgs.unblob\n</code></pre> <p>Or <code>nix profile</code>: <pre><code>$ nix profile install nixpkgs#unblob\n</code></pre></p> <p>Alternatively you can also install the latest version from git:</p> <p>Using the provided overlay:</p> <pre><code>{\n  nixpkgs.overlays = [\n    (let\n      unblob = import (builtins.fetchTarball {\n        url = \"https://github.com/onekey-sec/unblob/archive/main.tar.gz\";\n      });\n    in\n      \"{unblob}/overlay.nix\")\n  ];\n\n  environment.systemPackages = [ pkgs.unblob ]; # or `home.packages` for home-manager\n}\n</code></pre> <p>You can use the provided flake as well:</p> <pre><code>{\n  inputs.nixpkgs.url = \"...\";\n  inputs.unblob.url = \"github:onekey-sec/unblob\";\n\n  outputs = { nixpkgs, unblob, ...}: {\n    nixosConfiguration.&lt;my-host&gt; = nixpkgs.lib.nixosSystem {\n      modules = [\n        ({ pkgs, ... }: {\n          environment.systemPackages = [ unblob.packages.${pkgs.system}.default ];\n        })\n      ];\n    };\n  };\n}\n</code></pre> <p>For imperative package installation, follow these steps:</p> <ol> <li> <p>Optional: enable the experimental features so that you don't need to pass <code>--extra-experimental-features \"nix-command flakes\"</code> to <code>nix</code> command invocations:</p> <pre><code>  mkdir -p ~/.config/nix\n  cat &gt; ~/.config/nix/nix.conf &lt;&lt;EOF\n  experimental-features = nix-command flakes\n  EOF\n</code></pre> </li> <li> <p>Install unblob:</p> <pre><code>$ nix profile install github:onekey-sec/unblob\ndo you want to allow configuration setting 'extra-substituters' to be set to 'https://unblob.cachix.org' (y/N)? y\ndo you want to permanently mark this value as trusted (y/N)? y\ndo you want to allow configuration setting 'extra-trusted-public-keys' to be set to\n'unblob.cachix.org-1:5kWA6DwOg176rSqU8TOTBXWxsDB4LoCMfGfTgL5qCAE=' (y/N)? y\ndo you want to permanently mark this value as trusted (y/N)? y\n</code></pre> <p>Using and trusting substituter (binary cache) and its public key is optional but greatly speeds up installation.</p> </li> <li> <p>Check that everything works correctly:</p> <pre><code>unblob --show-external-dependencies\n</code></pre> </li> </ol>"},{"location":"installation/#from-source","title":"From source","text":"<ol> <li>Install Git if you don't have it yet.</li> <li>Install uv Python package manager.</li> <li> <p>Clone the unblob repository from GitHub:</p> <pre><code>git clone https://github.com/onekey-sec/unblob.git\n</code></pre> </li> <li> <p>Install Python dependencies with uv:</p> <ol> <li> <p>Python packages:</p> <pre><code>cd unblob\nuv sync --no-dev\n</code></pre> </li> <li> <p>Make sure you installed all extractors.</p> </li> <li> <p>Check that everything works correctly:</p> <pre><code>uv run unblob --show-external-dependencies\n</code></pre> </li> </ol> </li> </ol>"},{"location":"installation/#install-extractors","title":"Install extractors","text":"<p>There is a handy <code>install-deps.sh</code> script included in the repository and PyPI packages that can be used to install the following dependencies.</p> <ol> <li> <p>With your operating system package manager:     On Ubuntu 22.04, install extractors with APT:</p> <pre><code>sudo apt install android-sdk-libsparse-utils e2fsprogs p7zip-full unar zlib1g-dev liblzo2-dev lzop lziprecover libhyperscan-dev zstd lz4\n</code></pre> </li> <li> <p>If you need squashfs support, install sasquatch:</p> <pre><code>curl -L -o sasquatch_1.0.deb \"https://github.com/onekey-sec/sasquatch/releases/download/sasquatch-v4.5.1-5/sasquatch_1.0_$(dpkg --print-architecture).deb\"\nsudo dpkg -i sasquatch_1.0.deb\nrm sasquatch_1.0.deb\n</code></pre> </li> <li> <p>We maintain a fork of e2fsprogs based on Debian upstream, with some security fixes. You can install it this way:</p> <pre><code>curl -L -o e2fsprogs_1.47.0-3.ok2.deb \"https://github.com/onekey-sec/e2fsprogs/releases/download/v1.47.0-3.ok2/e2fsprogs_1.47.0-3.ok2_$(dpkg --print-architecture).deb\"\ncurl -L -o libext2fs2_1.47.0-3.ok2.deb \"https://github.com/onekey-sec/e2fsprogs/releases/download/v1.47.0-3.ok2/libext2fs2_1.47.0-3.ok2_$(dpkg --print-architecture).deb\"\ncurl -L -o libss2_1.47.0-3.ok2.deb \"https://github.com/onekey-sec/e2fsprogs/releases/download/v1.47.0-3.ok2/libss2_1.47.0-3.ok2_$(dpkg --print-architecture).deb\"\nsudo dpkg -i libext2fs2_1.47.0-3.ok2.deb libss2_1.47.0-3.ok2.deb\nsudo dpkg -i e2fsprogs_1.47.0-3.ok2.deb\nrm -f libext2fs2_1.47.0-3.ok2.deb libss2_1.47.0-3.ok2.deb e2fsprogs_1.47.0-3.ok2.de\n</code></pre> </li> </ol> <p>In case you already had e2fsprogs installed, you might need to upgrade some more packages from e2fsprogs.    You can get the names of the installed e2fsprogs binary packages this way:</p> <pre><code>    sudo dpkg-query -W -f '${db:Status-Abbrev}\\t${source:Package}\\t${Package}\\n' | grep '^i...e2fsprogs' | cut -f3\n</code></pre>"},{"location":"privacy/","title":"Privacy Policy","text":""},{"location":"privacy/#tldr","title":"TLDR","text":"<ul> <li>The website owner doesn't steal or sell any of your data.</li> <li>The website uses Google Analytics in a GDPR compliant way.</li> <li>Cookies are used where necessary.</li> <li>Local storage is used where necessary.</li> <li>The site is hosted by GitHub Pages.</li> </ul>"},{"location":"privacy/#introduction","title":"Introduction","text":"<p>This policy sets out the different areas where user privacy is concerned and outlines the obligations &amp; requirements of the users, the website and website owner. Furthermore the way this website processes, stores and protects user data and information will also be detailed within this policy.</p>"},{"location":"privacy/#the-website","title":"The Website","text":"<p>This website and its owner take a proactive approach to user privacy and ensure the necessary steps are taken to protect the privacy of its users throughout their visiting experience.</p>"},{"location":"privacy/#access-logs","title":"Access logs","text":"<p>The service hosting the unblob.org website stores access logs. This data is not accessible to and not readable for the owner of this website. The hosting service, GitHub, has their privacy policy is available here: https://docs.github.com/en/github/site-policy/github-privacy-statement</p>"},{"location":"privacy/#about-cookies","title":"About cookies","text":""},{"location":"privacy/#cookie-consent","title":"Cookie consent","text":"<p>This website uses cookies. By using this website and agreeing to this policy, you consent to unblob.org's use of cookies in accordance with the terms of this policy. Cookies are information packets sent by web servers to web browsers, and stored by the web browsers. The information is then sent back to the server each time the browser requests a page from the server. This enables a web server to identify and track web browsers.</p> <p>In order to comply with your preferences, the website may store a small number tiny functional cookies so that you're not asked to make this choice again, to be able to save your settings and to warn you of an updated changelog. Please note that these cookies are only used to guarantee a user-friendly visit and a proper website performance.</p>"},{"location":"privacy/#google-analytics","title":"Google Analytics","text":"<p>unblob.org uses Google Analytics to analyse the use of this website. Google Analytics generates statistical and other information about website use by means of cookies. The information generated relating to this website is used to create reports about the use of the website. Google will store and use this information. Google's privacy policy is available at: https://www.google.com/policies/privacy. To opt-out Of Google Analytics Google created a browser add-on that allows you to turn off Google Analytics trackers, and works for Chrome, Firefox, Safari, Opera, and even Internet Explorer 11.</p>"},{"location":"privacy/#refusing-cookies","title":"Refusing cookies","text":"<p>If you wish to disable cookies, you may do so through your individual browser options. More detailed information about cookie management with specific web browsers can be found at the browsers' respective websites.</p>"},{"location":"privacy/#local-storage","title":"Local Storage","text":"<p>Certain pages on this website may use your browser's local storage to allow you to store some history. This data is not available to the website owner, or sent to any third parties.</p>"},{"location":"privacy/#external-links","title":"External links","text":"<p>Although this website only looks to include quality, safe and relevant external links users should always adopt a policy of caution before clicking any external web links mentioned throughout this website. The owner of this website cannot guarantee or verify the contents of any externally linked website despite his best efforts. Users should therefore note they click on external links at their own risk and this website and its owner cannot be held liable for any damages or implications caused by visiting any external links mentioned.</p>"},{"location":"privacy/#changes-to-policy","title":"Changes to policy","text":"<p>Changes in this policy will be posted on this page.  You are advised to check this page regularly to view the most recent privacy policy.</p>"},{"location":"privacy/#kudos","title":"Kudos","text":"<p>This privacy policy was inspired by erresen.github.io</p>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/#talks","title":"Talks","text":"<ul> <li>BlackAlps 2022 - Firmwares Are Weird. A Year Long Journey To Efficient Extraction (video)</li> <li>BlackAlps 2022 - Firmwares Are Weird. A Year Long Journey To Efficient Extraction (PDF)</li> <li>Hacktivity Budapest 2022 - Making Sense of Firmware Images - The Journey to Efficient Extraction (video)</li> </ul>"},{"location":"publications/#demo-workshop","title":"Demo &amp; Workshop","text":"<ul> <li>BlackHat EU Arsenal 2023</li> <li>IT-S NOW Vienna 2023</li> <li>BlackHat Asia Arsenal 2023</li> <li>DEFCON Demo Labs 2022</li> <li>BlackHat USA Arsenal 2022</li> </ul>"},{"location":"publications/#podcasts","title":"Podcasts","text":"<ul> <li>SecTools Podcast - Episode 41 with ONEKEY Team about Unblob</li> </ul>"},{"location":"support/","title":"Support","text":""},{"location":"support/#commercial-support","title":"Commercial support","text":"<p>unblob is maintained by ONEKEY.</p> <p>For professional support or commercial enquiries, feel free to contact us via our website or write an email to support@onekey.com.</p> <p>If you want to use unblob within your research projects, don't hesitate to get in touch with us! The research team is directly reachable at research@onekey.com.</p>"},{"location":"support/#community-support","title":"Community support","text":"<p>To report bugs, feature requests, odd errors or behaviors, or recommend other enhancements, do not hesitate to use our issue tracker.</p> <p>In absolutely no way should you open an issue in our issue tracker if you found a security issue within unblob. If you found one, please follow our security policy guidelines!</p> <p>If you have a question or want to start a longer discussion, or you are not sure how to use unblob or any other topic, you can open a Discussion on GitHub.</p>"}]}